import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import anthropic
import base64
from io import BytesIO
from PIL import Image
import re
import os
import json
import hashlib
from typing import Any
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

ANTHROPIC_VERSION = "2023-06-01"


def get_api_key_from_env() -> str:
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—"""
    return os.getenv("ANTHROPIC_API_KEY", "")


def get_api_key_from_secrets() -> str:
    """Streamlit Secretsã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—"""
    try:
        return st.secrets.get("ANTHROPIC_API_KEY", "")
    except Exception:
        return ""


def get_stored_api_key() -> str:
    """ç’°å¢ƒå¤‰æ•°ã€Secretsã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®é †ã§APIã‚­ãƒ¼ã‚’å–å¾—"""
    # ã¾ãšç’°å¢ƒå¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    env_key = get_api_key_from_env()
    if env_key:
        return env_key
    # æ¬¡ã«Secretsã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆStreamlit Cloudç”¨ï¼‰
    secrets_key = get_api_key_from_secrets()
    if secrets_key:
        return secrets_key
    # æœ€å¾Œã«ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼‰
    return st.session_state.get("user_api_key", "")


st.set_page_config(
    page_title="æ¼«ç”»ç”»åƒè§£æãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ“š",
    layout="wide"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .stCheckbox p {
        font-size: 14px;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ“š æ¼«ç”»ç”»åƒè§£æãƒ„ãƒ¼ãƒ«")
st.markdown("URLã‹ã‚‰æ¼«ç”»ç”»åƒã‚’æŠ½å‡ºã—ã€AIã§ã‚ã‚‰ã™ã˜ã‚’è§£æã—ã¾ã™")


# ----------------------------
# ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®åŸºæœ¬æ–¹é‡
# - ç”»åƒã¯ç¸®å°ï¼‹JPEGåŒ–ã—ã¦é€ä¿¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‰Šæ¸›
# - ç”»åƒæŠ½å‡ºã¯å®‰ä¾¡ãƒ¢ãƒ‡ãƒ«ã§å®Ÿæ–½ã—ã€æ€ªã—ã„çµæœã®ã¿Opusã¸ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
# - è¦ç´„ãƒ»æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã¯åŸå‰‡ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ãªã®ã§å®‰ä¾¡ãƒ¢ãƒ‡ãƒ«ã¸
# - Streamlitã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§åŒä¸€å…¥åŠ›ã®å†èª²é‡‘ã‚’æŠ‘æ­¢
# ----------------------------


def _sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def _safe_json_loads(s: str) -> dict[str, Any] | None:
    try:
        return json.loads(s)
    except Exception:
        return None


def _extract_json_block(text: str) -> str | None:
    """ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‹ã‚‰JSONéƒ¨åˆ†ã ã‘ã‚’æŠœãå‡ºã™ï¼ˆå‰å¾Œã«èª¬æ˜æ–‡ãŒä»˜ãã“ã¨ãŒã‚ã‚‹ãŸã‚ï¼‰"""
    if not text:
        return None
    # æœ€åˆã® { ã‹ã‚‰æœ€å¾Œã® } ã‚’é›‘ã«æ‹¾ã†ï¼ˆæœ€å°é™ã®å®Ÿè£…ï¼‰
    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1 or end <= start:
        return None
    return text[start : end + 1]


@st.cache_data(show_spinner=False, ttl=60 * 60)
def _cached_download_image(url: str, referer: str = "") -> bytes | None:
    return download_image(url, referer)


def preprocess_image_bytes(
    img_bytes: bytes,
    max_side: int = 1024,
    jpeg_quality: int = 70,
) -> bytes:
    """ç”»åƒã‚’ç¸®å°ã—ã¦JPEGåŒ–ã—ã€é€ä¿¡ã‚³ã‚¹ãƒˆï¼ˆç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã‚’ä¸‹ã’ã‚‹"""
    img = Image.open(BytesIO(img_bytes))
    # é€éã‚’è€ƒæ…®ã—ã¦RGBã¸
    if img.mode not in ("RGB", "L"):
        img = img.convert("RGBA")
    if img.mode == "RGBA":
        bg = Image.new("RGBA", img.size, (255, 255, 255, 255))
        img = Image.alpha_composite(bg, img).convert("RGB")
    elif img.mode == "L":
        # ç™½é»’ã¯ãã®ã¾ã¾ã§ã‚‚ã‚ˆã„ãŒã€JPEGåŒ–ã®ãŸã‚RGBã¸çµ±ä¸€
        img = img.convert("RGB")

    w, h = img.size
    if max(w, h) > max_side:
        img.thumbnail((max_side, max_side))

    out = BytesIO()
    img.save(out, format="JPEG", quality=int(jpeg_quality), optimize=True, progressive=True)
    return out.getvalue()


def encode_image_to_base64_bytes(img_bytes: bytes) -> tuple[str, str]:
    base64_image = base64.standard_b64encode(img_bytes).decode("utf-8")
    return base64_image, "image/jpeg"


def call_claude_messages(
    api_key: str,
    model: str,
    content: list[dict],
    max_tokens: int,
    temperature: float = 0.2,
) -> str:
    """Claude APIå‘¼ã³å‡ºã—ï¼ˆå¤±æ•—æ™‚ã¯ä¾‹å¤–ã‚’æŠ•ã’ã‚‹ï¼‰"""
    client = anthropic.Anthropic(api_key=api_key)
    message = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        temperature=temperature,
        messages=[{"role": "user", "content": content}],
    )
    # contentãŒè¤‡æ•°è¦ç´ ã«ãªã‚‹ã‚±ãƒ¼ã‚¹ã¯ã‚ã‚‹ãŒã€ã“ã®ã‚¢ãƒ—ãƒªã§ã¯textå…ˆé ­ã§ååˆ†
    return message.content[0].text


def call_claude_messages_with_usage(
    api_key: str,
    model: str,
    content: list[dict],
    max_tokens: int,
    temperature: float = 0.2,
) -> tuple[str, dict[str, Any]]:
    """Claude APIå‘¼ã³å‡ºã—ï¼ˆusageã‚‚è¿”ã™ï¼‰"""
    client = anthropic.Anthropic(api_key=api_key)
    message = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        temperature=temperature,
        messages=[{"role": "user", "content": content}],
    )
    usage: dict[str, Any] = {}
    try:
        # anthropic SDKã®Messageã¯usageå±æ€§ã‚’æŒã¤
        u = getattr(message, "usage", None)
        if u is not None:
            usage = {
                "input_tokens": getattr(u, "input_tokens", None),
                "output_tokens": getattr(u, "output_tokens", None),
                "cache_creation_input_tokens": getattr(u, "cache_creation_input_tokens", None),
                "cache_read_input_tokens": getattr(u, "cache_read_input_tokens", None),
            }
    except Exception:
        usage = {}
    return message.content[0].text, usage


@st.cache_data(show_spinner=False, ttl=60 * 10)
def get_available_anthropic_models(api_key: str) -> list[str]:
    """Anthropic APIã‹ã‚‰åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ï¼ˆå–ã‚Œãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆï¼‰"""
    if not api_key:
        return []
    try:
        resp = requests.get(
            "https://api.anthropic.com/v1/models",
            headers={
                "x-api-key": api_key,
                "anthropic-version": ANTHROPIC_VERSION,
            },
            timeout=20,
        )
        if resp.status_code != 200:
            return []
        data = resp.json()
        models = []
        for item in data.get("data", []):
            mid = item.get("id")
            if mid:
                models.append(mid)
        # ã¤ã„ã§ã« "latest" ãŒã‚ã‚Œã°ä¸Šã«æ¥ã‚‹ã‚ˆã†ã«
        models = sorted(set(models), key=lambda s: (0 if "latest" in s else 1, s))
        return models
    except Exception:
        return []


def get_request_headers(url: str) -> dict:
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
    parsed_url = urlparse(url)
    base_domain = f"{parsed_url.scheme}://{parsed_url.netloc}"
    return {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Referer": base_domain,
    }


def get_pagination_urls(url: str, soup: BeautifulSoup, debug: bool = False) -> list[str]:
    """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®URLã‚’å–å¾—"""
    urls = [url]  # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã‚’å«ã‚€

    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³
    pagination_selectors = [
        ".pagination a",
        ".page-numbers a",
        ".pager a",
        ".wp-pagenavi a",
        "nav.navigation a",
        ".post-page-numbers",
        # æ•°å­—ãƒªãƒ³ã‚¯ï¼ˆ1, 2, 3...ï¼‰
        "a.page-link",
        ".pages a",
    ]

    pagination_links = []

    for selector in pagination_selectors:
        links = soup.select(selector)
        if links:
            pagination_links.extend(links)
            if debug:
                st.write(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º: {selector} ({len(links)}ä»¶)")
            break

    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€æ•°å­—ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
    if not pagination_links:
        # ãƒ†ã‚­ã‚¹ãƒˆãŒæ•°å­—ã®ã¿ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        all_links = soup.find_all("a")
        base_path = urlparse(url).path.rstrip('/')
        for link in all_links:
            text = link.get_text(strip=True)
            href = link.get("href", "")
            if not href:
                continue
            # æ•°å­—ã®ã¿ã®ãƒ†ã‚­ã‚¹ãƒˆ
            if text.isdigit():
                # çµ¶å¯¾URLã¾ãŸã¯ç›¸å¯¾URLã§åŒã˜è¨˜äº‹ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
                full_href = urljoin(url, href)
                href_path = urlparse(full_href).path.rstrip('/')
                # åŒã˜è¨˜äº‹ã¸ã®ãƒªãƒ³ã‚¯ï¼ˆ/archives/823243/2 å½¢å¼ï¼‰
                if href_path.startswith(base_path):
                    pagination_links.append(link)
                    if debug:
                        st.write(f"æ•°å­—ãƒªãƒ³ã‚¯æ¤œå‡º: {text} -> {full_href}")

    # URLã‚’æŠ½å‡º
    seen = {url}
    for link in pagination_links:
        href = link.get("href")
        if href:
            full_url = urljoin(url, href)
            # åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã€ã¾ã è¿½åŠ ã•ã‚Œã¦ã„ãªã„URL
            if urlparse(full_url).netloc == urlparse(url).netloc and full_url not in seen:
                # ã€Œæ¬¡ã¸ã€ã€Œå‰ã¸ã€ãªã©ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’é™¤å¤–
                text = link.get_text(strip=True).lower()
                if text not in ["next", "prev", "previous", "Â»", "Â«", "â€º", "â€¹", "æ¬¡ã¸", "å‰ã¸"]:
                    urls.append(full_url)
                    seen.add(full_url)

    # åŸºæœ¬URLã®ãƒ‘ã‚¹ã®é•·ã•ã‚’å–å¾—ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
    base_path = urlparse(url).path.rstrip('/')

    # URLã‚’ãƒšãƒ¼ã‚¸ç•ªå·é †ã«ã‚½ãƒ¼ãƒˆ
    def extract_page_num(u):
        path = urlparse(u).path.rstrip('/')
        # ãƒ™ãƒ¼ã‚¹URLã¨åŒã˜ãƒ‘ã‚¹ãªã‚‰1ãƒšãƒ¼ã‚¸ç›®
        if path == base_path:
            return 1
        # ãƒ™ãƒ¼ã‚¹URLã®å¾Œã«/æ•°å­—ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹: /archives/823243/2ï¼‰
        if path.startswith(base_path + '/'):
            suffix = path[len(base_path)+1:]
            if suffix.isdigit():
                return int(suffix)
        return 999  # ä¸æ˜ãªå ´åˆã¯æœ€å¾Œã«

    urls.sort(key=extract_page_num)

    if debug and len(urls) > 1:
        st.write(f"æ¤œå‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸: {len(urls)}ãƒšãƒ¼ã‚¸")
        for u in urls:
            st.write(f"  - {u}")

    return urls


def get_page_images(url: str, debug: bool = False) -> tuple[list[dict], BeautifulSoup]:
    """ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒURLã‚’æŠ½å‡º"""
    headers = get_request_headers(url)

    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
    except requests.RequestException as e:
        st.error(f"ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return [], None

    soup = BeautifulSoup(response.content, "html.parser")
    images = []

    if debug:
        st.write(f"HTMLã‚µã‚¤ã‚º: {len(response.content)} bytes")

    # è¨˜äº‹æœ¬æ–‡å†…ã®ç”»åƒã‚’å„ªå…ˆçš„ã«å–å¾—
    content_selectors = [
        "article",
        ".entry-content",
        ".post-content",
        ".article-content",
        ".content",
        ".single-content",
        ".post-body",
        ".article-body",
        "main",
        "#content",
        "#main",
        ".post",
        ".entry",
        ".ystd",
        "#ystd",
    ]

    content_area = None
    for selector in content_selectors:
        content_area = soup.select_one(selector)
        if content_area:
            if debug:
                st.write(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢æ¤œå‡º: {selector}")
            break

    if not content_area:
        content_area = soup.body if soup.body else soup
        if debug:
            st.write("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢: bodyå…¨ä½“")

    img_tags = content_area.find_all("img")

    if debug:
        st.write(f"æ¤œå‡ºã•ã‚ŒãŸimgã‚¿ã‚°æ•°: {len(img_tags)}")

    skip_patterns = [
        "icon", "logo", "avatar", "emoji", "button",
        "banner", "advertisement", "widget",
        "gravatar", "favicon", "sprite", "pixel",
        "tracking", "analytics", "1x1"
    ]

    for img in img_tags:
        src = (
            img.get("src") or
            img.get("data-src") or
            img.get("data-lazy-src") or
            img.get("data-original") or
            img.get("data-full-url") or
            img.get("srcset", "").split()[0] if img.get("srcset") else None
        )

        if not src:
            continue

        if src.startswith("data:"):
            continue

        img_url = urljoin(url, src)

        img_extensions = [".jpg", ".jpeg", ".png", ".gif", ".webp"]
        has_img_ext = any(ext in img_url.lower() for ext in img_extensions)

        if any(pattern in img_url.lower() for pattern in skip_patterns):
            continue

        alt_text = img.get("alt", "")

        if has_img_ext or "/uploads/" in img_url or "/images/" in img_url:
            images.append({
                "url": img_url,
                "alt": alt_text
            })
            if debug:
                st.write(f"ç”»åƒè¿½åŠ : {img_url[:80]}...")

    # é‡è¤‡ã‚’é™¤å»
    seen_urls = set()
    unique_images = []
    for img in images:
        if img["url"] not in seen_urls:
            seen_urls.add(img["url"])
            unique_images.append(img)

    return unique_images, soup


def get_all_pages_images(url: str, debug: bool = False) -> list[dict]:
    """ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒã‚’å–å¾—"""
    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    first_page_images, soup = get_page_images(url, debug)

    if not soup:
        return []

    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º
    page_urls = get_pagination_urls(url, soup, debug)

    all_images = []
    seen_urls = set()

    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’è¿½åŠ 
    for img in first_page_images:
        if img["url"] not in seen_urls:
            img["page"] = 1
            all_images.append(img)
            seen_urls.add(img["url"])

    # è¿½åŠ ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆ
    if len(page_urls) > 1:
        for i, page_url in enumerate(page_urls[1:], start=2):
            if debug:
                st.write(f"ãƒšãƒ¼ã‚¸ {i} ã‚’å–å¾—ä¸­: {page_url}")

            page_images, _ = get_page_images(page_url, debug)

            for img in page_images:
                if img["url"] not in seen_urls:
                    img["page"] = i
                    all_images.append(img)
                    seen_urls.add(img["url"])

    return all_images


def get_next_episode_url(soup: BeautifulSoup, base_url: str, debug: bool = False) -> str | None:
    """ã€Œæ¬¡ã®è©±>>ã€ã®URLã‚’å–å¾—"""
    # <div class="page-text-body">æ¬¡ã®è©±ï¼ï¼</div> ã‚’æ¤œå‡º
    next_episode_div = soup.find("div", class_="page-text-body", string=lambda t: t and "æ¬¡ã®è©±" in t)

    if next_episode_div:
        # è¦ªè¦ç´ ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        parent = next_episode_div.find_parent("a")
        if parent and parent.get("href"):
            next_url = urljoin(base_url, parent["href"])
            if debug:
                st.write(f"ğŸ”— æ¬¡ã®è©±ã‚’æ¤œå‡º: {next_url}")
            return next_url

        # å…„å¼Ÿè¦ç´ ã‚„è¿‘ãã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        next_link = next_episode_div.find_next("a")
        if next_link and next_link.get("href"):
            next_url = urljoin(base_url, next_link["href"])
            if debug:
                st.write(f"ğŸ”— æ¬¡ã®è©±ã‚’æ¤œå‡º: {next_url}")
            return next_url

    if debug:
        st.write("â„¹ï¸ ã€Œæ¬¡ã®è©±ã€ãƒªãƒ³ã‚¯ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    return None


def get_episode_images(url: str, episode_num: int = 1, debug: bool = False) -> tuple[list[dict], str | None]:
    """1è©±åˆ†ã®ç”»åƒã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å«ã‚€ï¼‰

    Returns:
        tuple: (ç”»åƒãƒªã‚¹ãƒˆ, æ¬¡ã®è©±ã®URL or None)
    """
    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    first_page_images, soup = get_page_images(url, debug)

    if not soup:
        return [], None

    # ã€Œæ¬¡ã®è©±ã€ã®URLã‚’å–å¾—
    next_episode_url = get_next_episode_url(soup, url, debug)

    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º
    page_urls = get_pagination_urls(url, soup, debug)

    all_images = []
    seen_urls = set()

    if debug:
        st.write(f"ğŸ“– ç¬¬{episode_num}è©±ã®å–å¾—é–‹å§‹")

    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’è¿½åŠ 
    for img in first_page_images:
        if img["url"] not in seen_urls:
            img["page"] = 1
            img["episode"] = episode_num
            all_images.append(img)
            seen_urls.add(img["url"])

    # è¿½åŠ ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆ
    if len(page_urls) > 1:
        for i, page_url in enumerate(page_urls[1:], start=2):
            if debug:
                st.write(f"  ãƒšãƒ¼ã‚¸ {i} ã‚’å–å¾—ä¸­: {page_url}")

            page_images, page_soup = get_page_images(page_url, debug)

            for img in page_images:
                if img["url"] not in seen_urls:
                    img["page"] = i
                    img["episode"] = episode_num
                    all_images.append(img)
                    seen_urls.add(img["url"])

            # å„ãƒšãƒ¼ã‚¸ã§ã‚‚ã€Œæ¬¡ã®è©±ã€ãƒªãƒ³ã‚¯ã‚’ç¢ºèªï¼ˆæœ€å¾Œã®ãƒšãƒ¼ã‚¸ã§è¦‹ã¤ã‹ã‚‹ã“ã¨ãŒã‚ã‚‹ï¼‰
            if page_soup and not next_episode_url:
                next_episode_url = get_next_episode_url(page_soup, page_url, debug)

    if debug:
        st.write(f"ğŸ“– ç¬¬{episode_num}è©±: {len(all_images)}æšã®ç”»åƒã‚’å–å¾—")

    return all_images, next_episode_url


def get_multiple_episodes_images(url: str, num_episodes: int, debug: bool = False) -> list[dict]:
    """è¤‡æ•°è©±ã®ç”»åƒã‚’å–å¾—

    Args:
        url: é–‹å§‹è©±ã®URL
        num_episodes: å–å¾—ã™ã‚‹è©±æ•°
        debug: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰

    Returns:
        list: å…¨è©±ã®ç”»åƒãƒªã‚¹ãƒˆ
    """
    all_images = []
    current_url = url

    for episode in range(1, num_episodes + 1):
        if not current_url:
            if debug:
                st.write(f"âš ï¸ ç¬¬{episode}è©±ã®URLãŒã‚ã‚Šã¾ã›ã‚“ã€‚å–å¾—ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break

        if debug:
            st.write(f"ğŸ“š ç¬¬{episode}è©±ã‚’å–å¾—ä¸­: {current_url}")

        episode_images, next_url = get_episode_images(current_url, episode_num=episode, debug=debug)
        all_images.extend(episode_images)

        # æ¬¡ã®è©±ã¸
        current_url = next_url

        if not next_url and episode < num_episodes:
            if debug:
                st.write(f"â„¹ï¸ ç¬¬{episode}è©±ãŒæœ€çµ‚è©±ã§ã™ã€‚{episode}è©±åˆ†ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
            break

    if debug:
        st.write(f"âœ… åˆè¨ˆ {len(all_images)}æšã®ç”»åƒã‚’å–å¾—ï¼ˆ{min(episode, num_episodes)}è©±åˆ†ï¼‰")

    return all_images


def download_image(url: str, referer: str = "") -> bytes | None:
    """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
        "Referer": referer,
    }

    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.content
    except requests.RequestException:
        return None


def filter_manga_images(
    images: list[dict],
    min_size: int = 50000,
    referer: str = "",
    debug: bool = False,
    preprocess_max_side: int = 1024,
    preprocess_jpeg_quality: int = 70,
) -> list[dict]:
    """æ¼«ç”»ç”»åƒã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    manga_images = []

    for img_info in images:
        img_data = _cached_download_image(img_info["url"], referer)
        if not img_data:
            if debug:
                st.write(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {img_info['url'][:60]}...")
            continue

        if len(img_data) < min_size:
            if debug:
                st.write(f"ã‚µã‚¤ã‚ºä¸è¶³ ({len(img_data)} bytes): {img_info['url'][:60]}...")
            continue

        try:
            img = Image.open(BytesIO(img_data))
            width, height = img.size

            aspect_ratio = width / height if height > 0 else 0

            if aspect_ratio > 3:
                if debug:
                    st.write(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”é™¤å¤– ({aspect_ratio:.2f}): {img_info['url'][:60]}...")
                continue

            if width < 200 or height < 200:
                if debug:
                    st.write(f"ã‚µã‚¤ã‚ºé™¤å¤– ({width}x{height}): {img_info['url'][:60]}...")
                continue

            # é€ä¿¡ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚ã€LLMé€ä¿¡ç”¨ã¯ç¸®å°ï¼‹JPEGåŒ–ã—ãŸã‚‚ã®ã‚’ä¿æŒ
            send_data = preprocess_image_bytes(
                img_data,
                max_side=preprocess_max_side,
                jpeg_quality=preprocess_jpeg_quality,
            )

            manga_images.append({
                **img_info,
                "data": img_data,
                "send_data": send_data,
                "width": width,
                "height": height,
                "size": len(img_data)
            })

            if debug:
                st.write(f"âœ… æ¼«ç”»ç”»åƒã¨ã—ã¦è¿½åŠ : {width}x{height}, raw={len(img_data)} bytes, send={len(send_data)} bytes")

        except Exception as e:
            if debug:
                st.write(f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return manga_images


def encode_image_to_base64(img_info: dict) -> tuple[str, str]:
    """ç”»åƒã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆLLMé€ä¿¡ç”¨ã®ç¸®å°JPEGã‚’å„ªå…ˆï¼‰"""
    img_bytes = img_info.get("send_data") or img_info.get("data") or b""
    return encode_image_to_base64_bytes(img_bytes)


def _get_llm_cache() -> dict:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆåŒä¸€ç”»åƒÃ—åŒä¸€ãƒ¢ãƒ‡ãƒ«ã®å†èª²é‡‘ã‚’æŠ‘æ­¢ï¼‰"""
    if "llm_cache" not in st.session_state:
        st.session_state["llm_cache"] = {}
    return st.session_state["llm_cache"]


def _image_cache_key(img_info: dict, model: str, prompt_key: str) -> str:
    img_bytes = img_info.get("send_data") or img_info.get("data") or b""
    h = hashlib.sha256(img_bytes).hexdigest()
    meta = f"{model}|{prompt_key}|ep={img_info.get('episode',1)}|p={img_info.get('page',1)}"
    return f"{h}:{_sha256_text(meta)}"


def _validate_image_facts(facts: dict[str, Any]) -> tuple[bool, list[str]]:
    """äººç‰©ãƒ»ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆã«å½±éŸ¿ã™ã‚‹èª¤ã‚Šã‚’æ‹¾ã†ãŸã‚ã®â€œæ€ªã—ã•â€åˆ¤å®šï¼ˆç”»åƒã¯è¦‹ãšãƒ†ã‚­ã‚¹ãƒˆã§åˆ¤å®šï¼‰"""
    reasons: list[str] = []
    if not isinstance(facts, dict):
        return False, ["JSONã§ã¯ã‚ã‚Šã¾ã›ã‚“"]

    confidence = facts.get("confidence")
    if isinstance(confidence, (int, float)):
        if confidence < 0.55:
            reasons.append(f"confidenceãŒä½ã„({confidence})")
    else:
        reasons.append("confidenceãŒæœªè¨­å®š")

    characters = facts.get("characters")
    events = facts.get("events")
    if not isinstance(characters, list):
        reasons.append("charactersãŒé…åˆ—ã§ã¯ãªã„")
        characters = []
    if not isinstance(events, list):
        reasons.append("eventsãŒé…åˆ—ã§ã¯ãªã„")
        events = []

    # äººç‰©ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆãŒä¸¡æ–¹ç©ºã¯å±é™ºï¼ˆèª­ã¿å–ã‚Šå¤±æ•—ã®å¯èƒ½æ€§ãŒé«˜ã„ï¼‰
    if len(characters) == 0 and len(events) == 0:
        reasons.append("äººç‰©/ã‚¤ãƒ™ãƒ³ãƒˆãŒç©º")

    # â€œä¸æ˜â€ã‚„â€œèª­ã‚ãªã„â€ãŒå¤šã„å ´åˆã¯å±é™º
    as_text = json.dumps(facts, ensure_ascii=False)
    bad_markers = ["ä¸æ˜", "èª­ã‚ãªã„", "åˆ¤åˆ¥ä¸èƒ½", "è¦‹ãˆãªã„", "ã‚ã‹ã‚‰ãªã„", "?", "â–¡", "ï¿½"]
    if sum(as_text.count(m) for m in bad_markers) >= 3:
        reasons.append("ä¸æ˜/æ–‡å­—åŒ–ã‘/åˆ¤åˆ¥ä¸èƒ½ãŒå¤šã„")

    # ãƒ†ãƒ³ãƒ—ãƒ¬ã£ã½ã„å‡ºåŠ›ï¼ˆæ¥µç«¯ã«çŸ­ã„ï¼‰
    if len(as_text) < 120:
        reasons.append("å‡ºåŠ›ãŒçŸ­ã™ãã‚‹")

    suspicious = len(reasons) > 0
    return suspicious, reasons


def _add_usage_totals(meta: dict[str, Any], model: str, usage: dict[str, Any] | None) -> None:
    """usageã‚’ãƒ¢ãƒ‡ãƒ«åˆ¥ã«åˆç®—ã—ã¦metaã«ä¿å­˜"""
    if not model or not usage:
        return
    totals = meta.setdefault("usage_totals", {})
    m = totals.setdefault(model, {"input_tokens": 0, "output_tokens": 0, "calls": 0})
    it = usage.get("input_tokens")
    ot = usage.get("output_tokens")
    if isinstance(it, int):
        m["input_tokens"] += it
    if isinstance(ot, int):
        m["output_tokens"] += ot
    m["calls"] += 1


def extract_image_facts_single(
    img_info: dict,
    api_key: str,
    model: str,
    title: str = "",
    max_tokens: int = 700,
) -> dict[str, Any] | None:
    """ç”»åƒ1æšã‹ã‚‰â€œäººç‰©ãƒ»é–¢ä¿‚æ€§ãƒ»ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆâ€ã‚’æ§‹é€ åŒ–æŠ½å‡ºï¼ˆå¤§ç­‹ã‚ã‚‰ã™ã˜ç”¨é€”ï¼‰"""
    cache = _get_llm_cache()
    cache_key = _image_cache_key(img_info, model=model, prompt_key="facts_v1")
    if cache_key in cache:
        return cache[cache_key]

    base64_image, media_type = encode_image_to_base64(img_info)
    header = ""
    if title:
        header = f"å‚è€ƒã‚¿ã‚¤ãƒˆãƒ«: {title}\n"
    ep = img_info.get("episode", 1)
    page = img_info.get("page", 1)
    content = [
        {"type": "text", "text": f"{header}å¯¾è±¡: ç¬¬{ep}è©± P{page}\nä»¥ä¸‹ã®ç”»åƒã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯JSONã®ã¿ã€‚"},
        {
            "type": "image",
            "source": {"type": "base64", "media_type": media_type, "data": base64_image},
        },
        {
            "type": "text",
            "text": """è¦ä»¶:
- ç›®çš„ã¯ã€Œå¤§ç­‹ã®ã‚ã‚‰ã™ã˜ã€ã‚’ä½œã‚‹ã“ã¨ã€‚ã‚»ãƒªãƒ•ã®ä¸€å­—ä¸€å¥ã¯ä¸è¦ã€‚
- ãŸã ã—ã€Œç™»å ´äººç‰©/é–¢ä¿‚æ€§ã€ã€Œä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆã€ã¯å–ã‚Šé•ãˆã‚‹ã¨è‡´å‘½çš„ãªã®ã§æ…é‡ã«ã€‚
- æ¨æ¸¬ã§è£œå®Œã—ãªã„ã€‚èª­ã‚ãªã„/ä¸æ˜ã¯ä¸æ˜ã¨æ›¸ãã€‚

å‡ºåŠ›(JSONã®ã¿):
{
  "episode": <int>,
  "page": <int>,
  "characters": [
    {"name_or_role": "<äººç‰©åã¾ãŸã¯å½¹å‰²>", "relation_terms": ["ç¾©æ¯","å¤«",...], "evidence": "<æ ¹æ‹ (çŸ­ã„å¼•ç”¨oræå†™)>"} 
  ],
  "events": ["<ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆ1>","<ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆ2>"],
  "key_dialogue_quotes": ["<çŸ­ã„å¼•ç”¨(ä»»æ„)>"],
  "confidence": <0.0-1.0>,
  "uncertainties": ["<ä¸ç¢ºã‹ãªç‚¹>"]
}""",
        },
    ]

    try:
        text, usage = call_claude_messages_with_usage(
            api_key=api_key,
            model=model,
            content=content,
            max_tokens=max_tokens,
            temperature=0.2,
        )
    except Exception:
        cache[cache_key] = None
        return None

    json_block = _extract_json_block(text) or text
    facts = _safe_json_loads(json_block)
    if isinstance(facts, dict):
        # è¶³ã‚Šãªã„ãƒ¡ã‚¿ã‚’è£œå®Œ
        facts.setdefault("episode", ep)
        facts.setdefault("page", page)
        facts["_usage"] = usage
        facts["_model"] = model
        cache[cache_key] = facts
        return facts

    cache[cache_key] = None
    return None


def extract_panel_details(
    images: list[dict],
    api_key: str,
    title: str = "",
    primary_model: str = "claude-sonnet-4-5-20251101",
    fallback_model: str = "claude-opus-4-5-20251101",
    max_tokens_per_image: int = 700,
    suspicious_confidence_threshold: float = 0.55,
    enable_text_verifier: bool = True,
    verifier_model: str = "claude-haiku-4-5-20251101",
    debug: bool = False,
) -> tuple[str, dict[str, Any]]:
    """Step1: ç”»åƒâ†’äº‹å®ŸæŠ½å‡ºï¼ˆå®‰ä¾¡ãƒ¢ãƒ‡ãƒ«ä¸­å¿ƒã€æ€ªã—ã„ç”»åƒã ã‘Opusã¸ï¼‰

    Returns:
        panel_details_text: Step2ã¸æ¸¡ã™ãƒ†ã‚­ã‚¹ãƒˆ
        meta: ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä»¶æ•°ãªã©ã®ãƒ¡ã‚¿æƒ…å ±
    """
    extracted: list[dict[str, Any]] = []
    suspicious_indices: list[int] = []
    suspicious_reasons: dict[int, list[str]] = {}
    meta: dict[str, Any] = {"usage_totals": {}}

    for idx, img_info in enumerate(images):
        facts = extract_image_facts_single(
            img_info=img_info,
            api_key=api_key,
            model=primary_model,
            title=title,
            max_tokens=max_tokens_per_image,
        )
        if facts is None:
            suspicious_indices.append(idx)
            suspicious_reasons[idx] = ["æŠ½å‡ºå¤±æ•—(None)"]
            extracted.append({
                "episode": img_info.get("episode", 1),
                "page": img_info.get("page", 1),
                "characters": [],
                "events": [],
                "key_dialogue_quotes": [],
                "confidence": 0.0,
                "uncertainties": ["æŠ½å‡ºå¤±æ•—"],
            })
            continue

        # ã—ãã„å€¤åˆ¤å®š
        if isinstance(facts.get("confidence"), (int, float)) and facts["confidence"] < suspicious_confidence_threshold:
            suspicious, reasons = True, [f"confidence<{suspicious_confidence_threshold}"]
        else:
            suspicious, reasons = _validate_image_facts(facts)

        if suspicious:
            suspicious_indices.append(idx)
            suspicious_reasons[idx] = reasons

        extracted.append(facts)
        _add_usage_totals(meta, facts.get("_model", ""), facts.get("_usage"))

    # è¿½åŠ ã®æ¤œè¨¼ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã€å®‰ä¾¡ãƒ¢ãƒ‡ãƒ«ï¼‰: â€œæ€ªã—ã„ç”»åƒå€™è£œâ€ã‚’å¢—ã‚„ã™
    if enable_text_verifier and extracted:
        try:
            payload = json.dumps(extracted, ensure_ascii=False)
            verifier_content = [
                {
                    "type": "text",
                    "text": f"""ä»¥ä¸‹ã¯æ¼«ç”»ç”»åƒã‹ã‚‰æŠ½å‡ºã—ãŸJSONä¸€è¦§ã§ã™ã€‚
ç›®çš„ã¯ã€Œå¤§ç­‹ã®ã‚ã‚‰ã™ã˜ã€ã€‚ãŸã ã—ã€Œç™»å ´äººç‰©/é–¢ä¿‚æ€§ã€ã€Œä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆã€ã®å–ã‚Šé•ãˆã¯è‡´å‘½çš„ã§ã™ã€‚

ã“ã®JSONä¸€è¦§ã‚’èª­ã¿ã€æ˜ã‚‰ã‹ã«ä¸è¶³ãƒ»çŸ›ç›¾ãƒ»ãƒ†ãƒ³ãƒ—ãƒ¬è‡­ãƒ»ä¸è‡ªç„¶ãªé£›èºãŒã‚ã‚‹ç”»åƒ(é…åˆ—ã®index)ã‚’åˆ—æŒ™ã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯JSONã®ã¿: {{"suspicious_indexes":[0,2,...],"reasons":{{"0":["ç†ç”±1",...],...}}}}

å…¥åŠ›JSON:
{payload}"""
                }
            ]
            verifier_text, verifier_usage = call_claude_messages_with_usage(
                api_key=api_key,
                model=verifier_model,
                content=verifier_content,
                max_tokens=700,
                temperature=0.1,
            )
            _add_usage_totals(meta, verifier_model, verifier_usage)
            verifier_json = _safe_json_loads(_extract_json_block(verifier_text) or verifier_text)
            if isinstance(verifier_json, dict):
                extra = verifier_json.get("suspicious_indexes", [])
                if isinstance(extra, list):
                    for i in extra:
                        if isinstance(i, int) and 0 <= i < len(images) and i not in suspicious_indices:
                            suspicious_indices.append(i)
                            suspicious_reasons[i] = ["ãƒ†ã‚­ã‚¹ãƒˆæ¤œè¨¼ã§è¦å†ç¢ºèª"]
        except Exception:
            pass

    suspicious_indices = sorted(set(suspicious_indices))

    # æ€ªã—ã„ç”»åƒã ã‘Opusã¸å†æŠ½å‡ºï¼ˆä¸Šæ›¸ãï¼‰
    escalated = 0
    if fallback_model and fallback_model != primary_model:
        for idx in suspicious_indices:
            img_info = images[idx]
            facts_opus = extract_image_facts_single(
                img_info=img_info,
                api_key=api_key,
                model=fallback_model,
                title=title,
                max_tokens=max_tokens_per_image,
            )
            if facts_opus is not None:
                extracted[idx] = facts_opus
                _add_usage_totals(meta, facts_opus.get("_model", ""), facts_opus.get("_usage"))
                escalated += 1

    # Step2ã¸æ¸¡ã™â€œææ–™â€ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼ˆJSONã§ã‚‚ã‚ˆã„ãŒã€ã“ã“ã¯è¦‹ã‚„ã™ã•å„ªå…ˆï¼‰
    lines: list[str] = []
    if title:
        lines.append(f"ã€å‚è€ƒã‚¿ã‚¤ãƒˆãƒ«ã€‘{title}")
    lines.append("ã€ç”»åƒã”ã¨ã®æŠ½å‡ºï¼ˆäººç‰©/ã‚¤ãƒ™ãƒ³ãƒˆä¸­å¿ƒï¼‰ã€‘")
    for i, facts in enumerate(extracted, start=1):
        ep = facts.get("episode", 1)
        page = facts.get("page", 1)
        chars = facts.get("characters", [])
        events = facts.get("events", [])
        uq = facts.get("uncertainties", [])
        conf = facts.get("confidence", None)
        lines.append(f"\n### ç”»åƒ{i}ï¼ˆç¬¬{ep}è©± P{page}ï¼‰")
        lines.append(f"- confidence: {conf}")
        lines.append(f"- characters: {json.dumps(chars, ensure_ascii=False)}")
        lines.append(f"- events: {json.dumps(events, ensure_ascii=False)}")
        if uq:
            lines.append(f"- uncertainties: {json.dumps(uq, ensure_ascii=False)}")

    meta.update({
        "total_images": len(images),
        "suspicious_images": len(suspicious_indices),
        "escalated_to_opus": escalated,
        "suspicious_indices": suspicious_indices,
        "suspicious_reasons": suspicious_reasons,
        "primary_model": primary_model,
        "fallback_model": fallback_model,
        "verifier_model": verifier_model,
    })
    return "\n".join(lines), meta


def summarize_story(
    panel_details: str,
    api_key: str,
    title: str = "",
    model: str = "claude-opus-4-5-20251101",
) -> str:
    """Step2: æŠ½å‡ºã—ãŸæƒ…å ±ã‹ã‚‰ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ã¾ã¨ã‚ã‚‹"""
    try:
        prompt = f"""ä»¥ä¸‹ã¯æ¼«ç”»ç”»åƒã‹ã‚‰æŠ½å‡ºã—ãŸã€Œäººç‰©/é–¢ä¿‚æ€§/ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆã€ã®ææ–™ã§ã™ã€‚
ã“ã‚Œã‚’å…ƒã«ã€Œå¤§ç­‹ã®ã‚ã‚‰ã™ã˜ã€ã‚’ä½œã£ã¦ãã ã•ã„ã€‚

åˆ¶ç´„:
- æ¨æ¸¬ã§è£œå®Œã—ãªã„ã€‚ä¸æ˜ã¯ä¸æ˜ã¨ã—ã¦æ‰±ã†
- ãŸã ã—ã€äººç‰©/é–¢ä¿‚æ€§/ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆã‚’å–ã‚Šé•ãˆãªã„ï¼ˆçŸ›ç›¾ãŒã‚ã‚Œã°æ…é‡ã«ï¼‰
- æ„Ÿæƒ…è¡¨ç¾ã¯å¤šå°‘ã®å¹…ãŒã‚ã£ã¦ã‚ˆã„

"""
        if title:
            prompt += f"ã€å‚è€ƒã‚¿ã‚¤ãƒˆãƒ«ã€‘\n{title}\n\n"

        prompt += f"""ã€æŠ½å‡ºææ–™ã€‘
{panel_details}

---

å‡ºåŠ›å½¢å¼:
## ã‚ã‚‰ã™ã˜
(3ã€œ6æ–‡)

## ç™»å ´äººç‰©
(ç®‡æ¡æ›¸ãã€‚é–¢ä¿‚æ€§ã‚‚æ˜è¨˜)

## ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆ
(ç®‡æ¡æ›¸ãã€‚æ™‚ç³»åˆ—ãŒåˆ†ã‹ã‚‹ã‚ˆã†ã«)

## ä¸ç¢ºã‹ãªç‚¹
(ææ–™ã«ä¸æ˜/çŸ›ç›¾ãŒã‚ã‚‹å ´åˆã®ã¿)"""

        text = call_claude_messages(
            api_key=api_key,
            model=model,
            content=[{"type": "text", "text": prompt}],
            max_tokens=1300,
            temperature=0.2,
        )
        return text
    except Exception as e:
        return f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {str(e)}"


def analyze_images_batch(
    images: list[dict],
    api_key: str,
    title: str = "",
    primary_model: str = "claude-opus-4-5-20251101",
    fallback_model: str = "claude-opus-4-5-20251101",
    summary_model: str = "claude-opus-4-5-20251101",
    verifier_model: str = "claude-opus-4-5-20251101",
    max_tokens_per_image: int = 700,
    suspicious_confidence_threshold: float = 0.55,
    enable_text_verifier: bool = True,
    debug: bool = False,
) -> tuple[str, dict[str, Any]]:
    """2æ®µéšè§£æ: ã‚»ãƒªãƒ•æŠ½å‡ºâ†’ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã¾ã¨ã‚"""

    # Step 1: å„ç”»åƒã®ã‚»ãƒªãƒ•ãƒ»çŠ¶æ³ã‚’è©³ç´°ã«æŠ½å‡º
    panel_details, meta = extract_panel_details(
        images=images,
        api_key=api_key,
        title=title,
        primary_model=primary_model,
        fallback_model=fallback_model,
        max_tokens_per_image=max_tokens_per_image,
        suspicious_confidence_threshold=suspicious_confidence_threshold,
        enable_text_verifier=enable_text_verifier,
        verifier_model=verifier_model,
        debug=debug,
    )

    # Step 2: æŠ½å‡ºã—ãŸæƒ…å ±ã‹ã‚‰ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ã¾ã¨ã‚ã‚‹ï¼ˆusageã‚‚åé›†ï¼‰
    try:
        prompt = f"""ä»¥ä¸‹ã¯æ¼«ç”»ç”»åƒã‹ã‚‰æŠ½å‡ºã—ãŸã€Œäººç‰©/é–¢ä¿‚æ€§/ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆã€ã®ææ–™ã§ã™ã€‚
ã“ã‚Œã‚’å…ƒã«ã€Œå¤§ç­‹ã®ã‚ã‚‰ã™ã˜ã€ã‚’ä½œã£ã¦ãã ã•ã„ã€‚

åˆ¶ç´„:
- æ¨æ¸¬ã§è£œå®Œã—ãªã„ã€‚ä¸æ˜ã¯ä¸æ˜ã¨ã—ã¦æ‰±ã†
- ãŸã ã—ã€äººç‰©/é–¢ä¿‚æ€§/ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆã‚’å–ã‚Šé•ãˆãªã„ï¼ˆçŸ›ç›¾ãŒã‚ã‚Œã°æ…é‡ã«ï¼‰
- æ„Ÿæƒ…è¡¨ç¾ã¯å¤šå°‘ã®å¹…ãŒã‚ã£ã¦ã‚ˆã„

"""
        if title:
            prompt += f"ã€å‚è€ƒã‚¿ã‚¤ãƒˆãƒ«ã€‘\n{title}\n\n"

        prompt += f"""ã€æŠ½å‡ºææ–™ã€‘
{panel_details}

---

å‡ºåŠ›å½¢å¼:
## ã‚ã‚‰ã™ã˜
(3ã€œ6æ–‡)

## ç™»å ´äººç‰©
(ç®‡æ¡æ›¸ãã€‚é–¢ä¿‚æ€§ã‚‚æ˜è¨˜)

## ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆ
(ç®‡æ¡æ›¸ãã€‚æ™‚ç³»åˆ—ãŒåˆ†ã‹ã‚‹ã‚ˆã†ã«)

## ä¸ç¢ºã‹ãªç‚¹
(ææ–™ã«ä¸æ˜/çŸ›ç›¾ãŒã‚ã‚‹å ´åˆã®ã¿)"""

        summary, summary_usage = call_claude_messages_with_usage(
            api_key=api_key,
            model=summary_model,
            content=[{"type": "text", "text": prompt}],
            max_tokens=1300,
            temperature=0.2,
        )
        _add_usage_totals(meta, summary_model, summary_usage)
    except Exception as e:
        summary = f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {str(e)}"

    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼ˆé•·æ–‡ã¯é‡ã„ã®ã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã¿ï¼‰
    meta["summary_model"] = summary_model
    meta["panel_details_preview"] = panel_details[:4000]

    return summary, meta


def check_title_consistency(
    title: str,
    summary: str,
    api_key: str,
    model: str = "claude-opus-4-5-20251101",
) -> str:
    """ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚ã‚‰ã™ã˜ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    prompt = f"""ä»¥ä¸‹ã®æ¼«ç”»è¨˜äº‹ã®ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ã¨ã€Œã‚ã‚‰ã™ã˜ã€ã‚’æ¯”è¼ƒã—ã¦ã€æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘
{title}

ã€ã‚ã‚‰ã™ã˜ã€‘
{summary}

---

ä»¥ä¸‹ã®è¦³ç‚¹ã§ãƒã‚§ãƒƒã‚¯ã—ã€çµæœã‚’å ±å‘Šã—ã¦ãã ã•ã„ï¼š

## æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯çµæœ

### åˆ¤å®š: [â—¯ æ•´åˆ / â–³ è»½å¾®ãªé•å’Œæ„Ÿ / âœ• ä¸æ•´åˆ]

### ãƒã‚§ãƒƒã‚¯é …ç›®

1. **ãƒ†ãƒ¼ãƒã®ä¸€è‡´**: ã‚¿ã‚¤ãƒˆãƒ«ãŒç¤ºã™ãƒ†ãƒ¼ãƒã¨ã‚ã‚‰ã™ã˜ã®å†…å®¹ã¯ä¸€è‡´ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ
2. **ç™»å ´äººç‰©**: ã‚¿ã‚¤ãƒˆãƒ«ã«äººç‰©ã‚„é–¢ä¿‚æ€§ãŒå«ã¾ã‚Œã‚‹å ´åˆã€ã‚ã‚‰ã™ã˜ã¨ä¸€è‡´ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ
3. **çµæœ«ãƒ»æ•™è¨“**: ã‚¿ã‚¤ãƒˆãƒ«ãŒç¤ºå”†ã™ã‚‹çµæœ«ã‚„æ•™è¨“ã¯ã€ã‚ã‚‰ã™ã˜ã«åæ˜ ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ
4. **èª‡å¤§è¡¨ç¾**: ã‚¿ã‚¤ãƒˆãƒ«ãŒå†…å®¹ã‚’èª‡å¼µã—ã™ãã¦ã„ã¾ã›ã‚“ã‹ï¼Ÿ

### è©³ç´°ã‚³ãƒ¡ãƒ³ãƒˆ
ï¼ˆé•å’Œæ„ŸãŒã‚ã‚‹å ´åˆã¯å…·ä½“çš„ã«æŒ‡æ‘˜ã—ã¦ãã ã•ã„ï¼‰

### æ”¹å–„ææ¡ˆ
ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®æ”¹å–„æ¡ˆãŒã‚ã‚Œã°ææ¡ˆã—ã¦ãã ã•ã„ï¼‰"""

    try:
        text = call_claude_messages(
            api_key=api_key,
            model=model,
            content=[{"type": "text", "text": prompt}],
            max_tokens=700,
            temperature=0.2,
        )
        return text
    except Exception as e:
        return f"ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}"


# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")

    # APIã‚­ãƒ¼ã®å–å¾—å…ƒã‚’ç¢ºèª
    env_key = get_api_key_from_env()
    secrets_key = get_api_key_from_secrets()

    if env_key:
        # .envã«è¨­å®šæ¸ˆã¿ã®å ´åˆ
        st.success("APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿ï¼ˆ.envï¼‰")
        api_key = env_key
    elif secrets_key:
        # Secretsã«è¨­å®šæ¸ˆã¿ã®å ´åˆ
        st.success("APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿ï¼ˆSecretsï¼‰")
        api_key = secrets_key
    else:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰
        api_key_input = st.text_input(
            "Anthropic API Key",
            type="password",
            value=st.session_state.get("user_api_key", ""),
            help="Claude APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            key="api_key_input"
        )

        if st.button("ğŸ” APIã‚­ãƒ¼ã‚’è¨­å®š", use_container_width=True):
            if api_key_input:
                st.session_state["user_api_key"] = api_key_input
                st.success("ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ã—ã¾ã—ãŸ")
                st.rerun()
            else:
                st.warning("APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

        if st.session_state.get("user_api_key"):
            st.info("APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰")
            if st.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢", use_container_width=True):
                del st.session_state["user_api_key"]
                st.rerun()

        api_key = st.session_state.get("user_api_key", "")

    st.divider()

    st.subheader("ç”»åƒãƒ•ã‚£ãƒ«ã‚¿è¨­å®š")
    min_image_size = st.slider(
        "æœ€å°ç”»åƒã‚µã‚¤ã‚º (KB)",
        min_value=1,
        max_value=500,
        value=30,
        help="ã“ã®å€¤ã‚ˆã‚Šå°ã•ã„ç”»åƒã¯é™¤å¤–ã•ã‚Œã¾ã™"
    )

    debug_mode = st.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰", value=True, help="ç”»åƒæ¤œå‡ºã®è©³ç´°ã‚’è¡¨ç¤º")

    st.divider()
    st.subheader("ğŸ’° ã‚³ã‚¹ãƒˆæœ€é©åŒ–ï¼ˆé‡è¦ï¼‰")

    st.markdown("**æ–¹é‡**: ç”»åƒã¯ç¸®å°ã—ã¦é€ä¿¡ã—ã€æŠ½å‡ºã¯å®‰ä¾¡ãƒ¢ãƒ‡ãƒ«ä¸­å¿ƒâ†’æ€ªã—ã„ç”»åƒã ã‘Opusã¸å†æŠ½å‡ºã—ã¾ã™ã€‚")

    preprocess_max_side = st.slider(
        "LLMé€ä¿¡ç”¨ ç”»åƒæœ€å¤§è¾º(px)",
        min_value=512,
        max_value=1600,
        value=1024,
        step=64,
        help="å°ã•ã„ã»ã©å®‰ããªã‚Šã‚„ã™ã„ï¼ˆãŸã ã—æ–‡å­—ãŒæ½°ã‚Œã‚‹ã¨ç²¾åº¦ä½ä¸‹ï¼‰"
    )
    preprocess_jpeg_quality = st.slider(
        "LLMé€ä¿¡ç”¨ JPEGå“è³ª",
        min_value=40,
        max_value=90,
        value=70,
        step=5,
        help="å°ã•ã„ã»ã©å®‰ããªã‚Šã‚„ã™ã„ï¼ˆãŸã ã—æ–‡å­—ãŒæ½°ã‚Œã‚‹ã¨ç²¾åº¦ä½ä¸‹ï¼‰"
    )
    max_images_total = st.slider(
        "è§£æã«ä½¿ã†æœ€å¤§ç”»åƒæšæ•°ï¼ˆä¸Šé™ï¼‰",
        min_value=5,
        max_value=120,
        value=45,
        step=5,
        help="å¤šã„ã»ã©ç²¾åº¦ã¯ä¸ŠãŒã‚Šå¾—ã¾ã™ãŒã€ã‚³ã‚¹ãƒˆãŒç›´ç·šçš„ã«å¢—ãˆã¾ã™"
    )

    st.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    available_models = get_available_anthropic_models(api_key) if api_key else []
    if available_models:
        st.caption("âœ… APIã‚­ãƒ¼ã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆ404ã‚’é¿ã‘ã‚‹ãŸã‚ã€ã“ã“ã‹ã‚‰é¸ã¶ã®ãŒãŠã™ã™ã‚ã§ã™ï¼‰")
    else:
        st.caption("â„¹ï¸ åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«åã¯æ‰‹å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ404ãŒå‡ºã‚‹å ´åˆã¯ãƒ¢ãƒ‡ãƒ«åãŒé•ã„ã¾ã™ï¼‰ã€‚")

    default_opus = "claude-opus-4-5-20251101"

    if available_models:
        primary_model = st.selectbox(
            "æŠ½å‡ºï¼ˆä¸€æ¬¡ï¼‰ãƒ¢ãƒ‡ãƒ«",
            options=available_models,
            index=0 if default_opus not in available_models else available_models.index(default_opus),
            help="ç”»åƒâ†’äººç‰©/ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡ºã®ä¸€æ¬¡ãƒ¢ãƒ‡ãƒ«ï¼ˆåŸºæœ¬ã¯ã“ã“ã‚’å®‰ä¾¡ã«ï¼‰"
        )
    else:
        primary_model = st.text_input(
            "æŠ½å‡ºï¼ˆä¸€æ¬¡ï¼‰ãƒ¢ãƒ‡ãƒ«",
            value=default_opus,
            help="ç”»åƒâ†’äººç‰©/ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡ºã®ä¸€æ¬¡ãƒ¢ãƒ‡ãƒ«ï¼ˆåŸºæœ¬ã¯ã“ã“ã‚’å®‰ä¾¡ã«ï¼‰"
        )
    enable_fallback_opus = st.checkbox("æ€ªã—ã„ç”»åƒã ã‘é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã¸å†æŠ½å‡ºï¼ˆæ¨å¥¨ï¼‰", value=True)
    if available_models:
        fallback_model = st.selectbox(
            "æŠ½å‡ºï¼ˆå†æŠ½å‡ºï¼‰ãƒ¢ãƒ‡ãƒ«",
            options=available_models,
            index=0 if default_opus not in available_models else available_models.index(default_opus),
            help="ä¸€æ¬¡æŠ½å‡ºãŒæ€ªã—ã„æ™‚ã ã‘ä½¿ã†ãƒ¢ãƒ‡ãƒ«ï¼ˆOpusãªã©ï¼‰"
        )
    else:
        fallback_model = st.text_input(
            "æŠ½å‡ºï¼ˆå†æŠ½å‡ºï¼‰ãƒ¢ãƒ‡ãƒ«",
            value=default_opus,
            help="ä¸€æ¬¡æŠ½å‡ºãŒæ€ªã—ã„æ™‚ã ã‘ä½¿ã†ãƒ¢ãƒ‡ãƒ«ï¼ˆOpusãªã©ï¼‰"
        )

    if available_models:
        summary_model = st.selectbox(
            "è¦ç´„ãƒ¢ãƒ‡ãƒ«",
            options=available_models,
            index=0 if default_opus not in available_models else available_models.index(default_opus),
            help="ç”»åƒæŠ½å‡ºå¾Œã®ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ãªã®ã§ã€åŸºæœ¬ã¯å®‰ä¾¡ãƒ¢ãƒ‡ãƒ«ã§OKï¼ˆå®‰ã„ãƒ¢ãƒ‡ãƒ«ãŒä½¿ãˆã‚‹ãªã‚‰åˆ‡æ›¿æ¨å¥¨ï¼‰"
        )
        verifier_model = st.selectbox(
            "ãƒ†ã‚­ã‚¹ãƒˆæ¤œè¨¼ãƒ¢ãƒ‡ãƒ«ï¼ˆä»»æ„ï¼‰",
            options=available_models,
            index=0 if default_opus not in available_models else available_models.index(default_opus),
            help="æŠ½å‡ºJSONã®ä¸è¶³/çŸ›ç›¾ã‚’ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã§æ¤œçŸ¥ï¼ˆå®‰ã„ãƒ¢ãƒ‡ãƒ«æ¨å¥¨ï¼‰"
        )
        consistency_model = st.selectbox(
            "ã‚¿ã‚¤ãƒˆãƒ«æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«",
            options=available_models,
            index=0 if default_opus not in available_models else available_models.index(default_opus),
            help="ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®ãƒã‚§ãƒƒã‚¯ã€‚å®‰ã„ãƒ¢ãƒ‡ãƒ«ã§ååˆ†"
        )
    else:
        summary_model = st.text_input(
            "è¦ç´„ãƒ¢ãƒ‡ãƒ«",
            value=default_opus,
            help="ç”»åƒæŠ½å‡ºå¾Œã®ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ãªã®ã§ã€åŸºæœ¬ã¯å®‰ä¾¡ãƒ¢ãƒ‡ãƒ«ã§OKï¼ˆå®‰ã„ãƒ¢ãƒ‡ãƒ«ãŒä½¿ãˆã‚‹ãªã‚‰åˆ‡æ›¿æ¨å¥¨ï¼‰"
        )
        verifier_model = st.text_input(
            "ãƒ†ã‚­ã‚¹ãƒˆæ¤œè¨¼ãƒ¢ãƒ‡ãƒ«ï¼ˆä»»æ„ï¼‰",
            value=default_opus,
            help="æŠ½å‡ºJSONã®ä¸è¶³/çŸ›ç›¾ã‚’ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã§æ¤œçŸ¥ï¼ˆå®‰ã„ãƒ¢ãƒ‡ãƒ«æ¨å¥¨ï¼‰"
        )
        consistency_model = st.text_input(
            "ã‚¿ã‚¤ãƒˆãƒ«æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«",
            value=default_opus,
            help="ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®ãƒã‚§ãƒƒã‚¯ã€‚å®‰ã„ãƒ¢ãƒ‡ãƒ«ã§ååˆ†"
        )

    st.subheader("ğŸ” æ¤œçŸ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    max_tokens_per_image = st.slider(
        "ç”»åƒ1æšã‚ãŸã‚Šã®æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³",
        min_value=200,
        max_value=1400,
        value=700,
        step=50,
        help="å¤§ãã„ã»ã©æƒ…å ±ãŒå¢—ãˆã‚‹å¯èƒ½æ€§ã¯ã‚ã‚‹ãŒã€ã‚³ã‚¹ãƒˆã‚‚å¢—ãˆã‚‹"
    )
    suspicious_confidence_threshold = st.slider(
        "confidenceã—ãã„å€¤ï¼ˆã“ã‚Œæœªæº€ã¯å†æŠ½å‡ºå€™è£œï¼‰",
        min_value=0.30,
        max_value=0.80,
        value=0.55,
        step=0.05,
    )
    enable_text_verifier = st.checkbox(
        "ãƒ†ã‚­ã‚¹ãƒˆæ¤œè¨¼ã§â€œæ€ªã—ã„ç”»åƒâ€å€™è£œã‚’è¿½åŠ ï¼ˆæ¨å¥¨ï¼‰",
        value=True,
        help="ç”»åƒã¯è¦‹ãšã€æŠ½å‡ºçµæœ(JSON)ã ã‘ã‚’å®‰ä¾¡ãƒ¢ãƒ‡ãƒ«ã§ãƒã‚§ãƒƒã‚¯ã—ã¾ã™"
    )

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
st.subheader("ğŸ“– æ¼«ç”»ã‚¿ã‚¤ãƒ—ã‚’é¸æŠã—ã¦URLã‚’å…¥åŠ›")

manga_type_col1, manga_type_col2 = st.columns(2)

with manga_type_col1:
    st.markdown("**ğŸ“š é€£è¼‰æ¼«ç”»**ï¼ˆ3è©±åˆ†èª­ã¿è¾¼ã¿ï¼‰")
    serial_url = st.text_input(
        "é€£è¼‰æ¼«ç”»URL",
        placeholder="https://example.com/serial-manga",
        help="é€£è¼‰æ¼«ç”»ã®URLã‚’å…¥åŠ›ï¼ˆ3è©±åˆ†ã®ç”»åƒã‚’èª­ã¿è¾¼ã¿ã¾ã™ï¼‰",
        label_visibility="collapsed"
    )

with manga_type_col2:
    st.markdown("**ğŸ“„ ã‚¨ãƒ”æ¼«ç”»**ï¼ˆ1è©±åˆ†èª­ã¿è¾¼ã¿ï¼‰")
    episode_url = st.text_input(
        "ã‚¨ãƒ”æ¼«ç”»URL",
        placeholder="https://example.com/episode-manga",
        help="ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¼«ç”»ã®URLã‚’å…¥åŠ›ï¼ˆ1è©±åˆ†ã®ç”»åƒã‚’èª­ã¿è¾¼ã¿ã¾ã™ï¼‰",
        label_visibility="collapsed"
    )

# ã©ã¡ã‚‰ã®URLãŒå…¥åŠ›ã•ã‚ŒãŸã‹åˆ¤å®š
url = ""
num_episodes = 1
manga_type_label = ""

if serial_url and episode_url:
    st.warning("âš ï¸ ã©ã¡ã‚‰ã‹ä¸€æ–¹ã®URLã®ã¿å…¥åŠ›ã—ã¦ãã ã•ã„")
elif serial_url:
    url = serial_url
    num_episodes = 3
    manga_type_label = "é€£è¼‰æ¼«ç”»"
elif episode_url:
    url = episode_url
    num_episodes = 1
    manga_type_label = "ã‚¨ãƒ”æ¼«ç”»"

article_title = st.text_input(
    "ğŸ“° è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä»»æ„ï¼‰",
    placeholder="æ¼«ç”»è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›",
    help="ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã‚ã‚‰ã™ã˜ã¨ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™"
)

col1, col2 = st.columns([1, 4])
with col1:
    analyze_button = st.button("ğŸ” è§£æé–‹å§‹", type="primary", use_container_width=True)

if analyze_button:
    if not url:
        st.error("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆé€£è¼‰æ¼«ç”»ã¾ãŸã¯ã‚¨ãƒ”æ¼«ç”»ã®ã©ã¡ã‚‰ã‹ï¼‰")
    elif serial_url and episode_url:
        st.error("ã©ã¡ã‚‰ã‹ä¸€æ–¹ã®URLã®ã¿å…¥åŠ›ã—ã¦ãã ã•ã„")
    elif not api_key:
        st.error("APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    else:
        st.info(f"ğŸ“– **{manga_type_label}**ã¨ã—ã¦è§£æã—ã¾ã™ï¼ˆ{num_episodes}è©±åˆ†ï¼‰")

        with st.spinner("ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒã‚’å–å¾—ä¸­..."):
            # æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯: è©±æ•°å˜ä½ã§å–å¾—
            images = get_multiple_episodes_images(url, num_episodes=num_episodes, debug=debug_mode)

        if not images:
            st.warning("ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’ONã«ã—ã¦è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            st.info(f"ğŸ“· {len(images)}ä»¶ã®ç”»åƒã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚æ¼«ç”»ç”»åƒã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...")

            with st.spinner("æ¼«ç”»ç”»åƒã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­..."):
                manga_images = filter_manga_images(
                    images,
                    min_size=min_image_size * 1000,
                    referer=url,
                    debug=debug_mode,
                    preprocess_max_side=preprocess_max_side,
                    preprocess_jpeg_quality=preprocess_jpeg_quality,
                )

            if not manga_images:
                st.warning("æ¼«ç”»ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šã‚’èª¿æ•´ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")

                if debug_mode and images:
                    st.subheader("æ¤œå‡ºã•ã‚ŒãŸç”»åƒURLä¸€è¦§")
                    for img in images:
                        st.text(img["url"])
            else:
                # ã‚³ã‚¹ãƒˆæš´ç™ºé˜²æ­¢: æœ€å¤§æšæ•°ã§ã‚«ãƒƒãƒˆ
                if len(manga_images) > max_images_total:
                    st.warning(f"âš ï¸ ç”»åƒãŒ{len(manga_images)}æšã‚ã‚Šã¾ã™ã€‚ã‚³ã‚¹ãƒˆæŠ‘åˆ¶ã®ãŸã‚å…ˆé ­{max_images_total}æšã ã‘ã§è§£æã—ã¾ã™ã€‚")
                    manga_images = manga_images[:max_images_total]

                # è©±æ•°ã”ã¨ã®ç”»åƒæ•°ã‚’é›†è¨ˆ
                episode_counts = {}
                for img in manga_images:
                    ep = img.get("episode", 1)
                    episode_counts[ep] = episode_counts.get(ep, 0) + 1

                episode_summary = "ã€".join([f"ç¬¬{ep}è©±: {count}æš" for ep, count in sorted(episode_counts.items())])
                st.success(f"ğŸ“š {len(manga_images)}ä»¶ã®æ¼«ç”»ç”»åƒã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼ˆ{episode_summary}ï¼‰")

                # ç”»åƒã‚’è¡¨ç¤º
                st.header("ğŸ–¼ï¸ æ¤œå‡ºã•ã‚ŒãŸæ¼«ç”»ç”»åƒ")

                # ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºï¼ˆè©±æ•°ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰
                cols_per_row = 3
                for i in range(0, len(manga_images), cols_per_row):
                    cols = st.columns(cols_per_row)
                    for j, col in enumerate(cols):
                        idx = i + j
                        if idx < len(manga_images):
                            img_info = manga_images[idx]
                            with col:
                                page_num = img_info.get("page", 1)
                                episode_num = img_info.get("episode", 1)
                                st.image(
                                    img_info["data"],
                                    caption=f"ç¬¬{episode_num}è©± P{page_num}",
                                    use_container_width=True
                                )

                # ã‚ã‚‰ã™ã˜è§£æ
                st.divider()
                st.header("ğŸ“ ã‚ã‚‰ã™ã˜è§£æ")

                with st.spinner("AIãŒã‚ã‚‰ã™ã˜ã‚’è§£æä¸­..."):
                    used_fallback_model = fallback_model if enable_fallback_opus else primary_model
                    summary, meta = analyze_images_batch(
                        manga_images,
                        api_key,
                        title=article_title,
                        primary_model=primary_model,
                        fallback_model=used_fallback_model,
                        summary_model=summary_model,
                        verifier_model=verifier_model,
                        max_tokens_per_image=max_tokens_per_image,
                        suspicious_confidence_threshold=suspicious_confidence_threshold,
                        enable_text_verifier=enable_text_verifier,
                        debug=debug_mode,
                    )

                st.markdown(summary)

                with st.expander("ğŸ”§ è§£æãƒ¡ã‚¿ï¼ˆã‚³ã‚¹ãƒˆ/å“è³ªã®å‚è€ƒï¼‰", expanded=False):
                    st.write(f"ç·ç”»åƒ: {meta.get('total_images')} / æ€ªã—ã„åˆ¤å®š: {meta.get('suspicious_images')} / å†æŠ½å‡º: {meta.get('escalated_to_opus')}")
                    st.write(f"ä¸€æ¬¡ãƒ¢ãƒ‡ãƒ«: {meta.get('primary_model')}")
                    st.write(f"å†æŠ½å‡ºãƒ¢ãƒ‡ãƒ«: {meta.get('fallback_model')}")
                    st.write(f"æ¤œè¨¼ãƒ¢ãƒ‡ãƒ«: {meta.get('verifier_model')}")
                    st.write(f"è¦ç´„ãƒ¢ãƒ‡ãƒ«: {meta.get('summary_model')}")
                    idxs = meta.get("suspicious_indices", [])
                    if idxs:
                        st.write("æ€ªã—ã„ç”»åƒindexï¼ˆ0å§‹ã¾ã‚Šï¼‰:", idxs)
                        st.json(meta.get("suspicious_reasons", {}))
                    preview = meta.get("panel_details_preview")
                    if preview:
                        st.divider()
                        st.caption("æŠ½å‡ºææ–™ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­ã®ã¿ï¼‰: ã“ã“ãŒè–„ã„/ä¸æ˜ã ã¨ã€ã‚ã‚‰ã™ã˜ã‚‚è–„ããªã‚Šã¾ã™")
                        st.text_area("panel_details_preview", value=preview, height=240)

                    totals = meta.get("usage_totals")
                    if isinstance(totals, dict) and totals:
                        st.divider()
                        st.caption("usageé›†è¨ˆï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰: ã“ã“ã‹ã‚‰ã‚³ã‚¹ãƒˆã‚’æ¨å®šã§ãã¾ã™")
                        st.json(totals)

                        st.subheader("ğŸ’µ ã‚³ã‚¹ãƒˆæ¨å®šï¼ˆä»»æ„ï¼‰")
                        st.caption("å˜ä¾¡ã¯ã‚ãªãŸã®å¥‘ç´„/è«‹æ±‚å˜ä¾¡ã«åˆã‚ã›ã¦å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ$ / 1M tokensï¼‰ã€‚")
                        usd_jpy = st.number_input("æ›ç®—ãƒ¬ãƒ¼ãƒˆï¼ˆUSDâ†’JPYï¼‰", min_value=50.0, max_value=300.0, value=150.0, step=1.0)
                        default_in = st.number_input("å…¥åŠ›å˜ä¾¡ï¼ˆ$/1M tokensï¼‰(å…±é€š)", min_value=0.0, value=0.0, step=0.5)
                        default_out = st.number_input("å‡ºåŠ›å˜ä¾¡ï¼ˆ$/1M tokensï¼‰(å…±é€š)", min_value=0.0, value=0.0, step=0.5)

                        est_usd = 0.0
                        for mname, v in totals.items():
                            it = v.get("input_tokens", 0) or 0
                            ot = v.get("output_tokens", 0) or 0
                            if not isinstance(it, int):
                                it = 0
                            if not isinstance(ot, int):
                                ot = 0
                            est_usd += (it / 1_000_000.0) * float(default_in) + (ot / 1_000_000.0) * float(default_out)
                        st.write(f"æ¨å®šã‚³ã‚¹ãƒˆ: **ç´„ ${est_usd:.4f}ï¼ˆç´„ Â¥{est_usd * float(usd_jpy):.1f}ï¼‰**")

                # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                if article_title:
                    st.divider()
                    st.header("ğŸ” ã‚¿ã‚¤ãƒˆãƒ«æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯")

                    with st.spinner("ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚ã‚‰ã™ã˜ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ä¸­..."):
                        consistency_result = check_title_consistency(
                            article_title,
                            summary,
                            api_key,
                            model=consistency_model,
                        )

                    st.markdown(consistency_result)

# ãƒ•ãƒƒã‚¿ãƒ¼
st.divider()
st.caption("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã‚ã‚‰ã™ã˜ã¨ã®æ•´åˆæ€§ã‚’è‡ªå‹•ãƒã‚§ãƒƒã‚¯ã—ã¾ã™")

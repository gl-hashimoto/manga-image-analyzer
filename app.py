import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import anthropic
import base64
from io import BytesIO
from PIL import Image
import re
import os
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()


def get_api_key_from_env() -> str:
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—"""
    return os.getenv("ANTHROPIC_API_KEY", "")


def get_api_key_from_secrets() -> str:
    """Streamlit Secretsã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—"""
    try:
        return st.secrets.get("ANTHROPIC_API_KEY", "")
    except Exception:
        return ""


def get_stored_api_key() -> str:
    """ç’°å¢ƒå¤‰æ•°ã€Secretsã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®é †ã§APIã‚­ãƒ¼ã‚’å–å¾—"""
    # ã¾ãšç’°å¢ƒå¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    env_key = get_api_key_from_env()
    if env_key:
        return env_key
    # æ¬¡ã«Secretsã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆStreamlit Cloudç”¨ï¼‰
    secrets_key = get_api_key_from_secrets()
    if secrets_key:
        return secrets_key
    # æœ€å¾Œã«ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼‰
    return st.session_state.get("user_api_key", "")


st.set_page_config(
    page_title="æ¼«ç”»ç”»åƒè§£æãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ“š",
    layout="wide"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .stCheckbox p {
        font-size: 14px;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ“š æ¼«ç”»ç”»åƒè§£æãƒ„ãƒ¼ãƒ«")
st.markdown("URLã‹ã‚‰æ¼«ç”»ç”»åƒã‚’æŠ½å‡ºã—ã€AIã§ã‚ã‚‰ã™ã˜ã‚’è§£æã—ã¾ã™")


def get_request_headers(url: str) -> dict:
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
    parsed_url = urlparse(url)
    base_domain = f"{parsed_url.scheme}://{parsed_url.netloc}"
    return {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Referer": base_domain,
    }


def get_pagination_urls(url: str, soup: BeautifulSoup, debug: bool = False) -> list[str]:
    """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®URLã‚’å–å¾—"""
    urls = [url]  # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã‚’å«ã‚€

    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³
    pagination_selectors = [
        ".pagination a",
        ".page-numbers a",
        ".pager a",
        ".wp-pagenavi a",
        "nav.navigation a",
        ".post-page-numbers",
        # æ•°å­—ãƒªãƒ³ã‚¯ï¼ˆ1, 2, 3...ï¼‰
        "a.page-link",
        ".pages a",
    ]

    pagination_links = []

    for selector in pagination_selectors:
        links = soup.select(selector)
        if links:
            pagination_links.extend(links)
            if debug:
                st.write(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º: {selector} ({len(links)}ä»¶)")
            break

    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€æ•°å­—ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
    if not pagination_links:
        # ãƒ†ã‚­ã‚¹ãƒˆãŒæ•°å­—ã®ã¿ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        all_links = soup.find_all("a")
        base_path = urlparse(url).path.rstrip('/')
        for link in all_links:
            text = link.get_text(strip=True)
            href = link.get("href", "")
            if not href:
                continue
            # æ•°å­—ã®ã¿ã®ãƒ†ã‚­ã‚¹ãƒˆ
            if text.isdigit():
                # çµ¶å¯¾URLã¾ãŸã¯ç›¸å¯¾URLã§åŒã˜è¨˜äº‹ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
                full_href = urljoin(url, href)
                href_path = urlparse(full_href).path.rstrip('/')
                # åŒã˜è¨˜äº‹ã¸ã®ãƒªãƒ³ã‚¯ï¼ˆ/archives/823243/2 å½¢å¼ï¼‰
                if href_path.startswith(base_path):
                    pagination_links.append(link)
                    if debug:
                        st.write(f"æ•°å­—ãƒªãƒ³ã‚¯æ¤œå‡º: {text} -> {full_href}")

    # URLã‚’æŠ½å‡º
    seen = {url}
    for link in pagination_links:
        href = link.get("href")
        if href:
            full_url = urljoin(url, href)
            # åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã€ã¾ã è¿½åŠ ã•ã‚Œã¦ã„ãªã„URL
            if urlparse(full_url).netloc == urlparse(url).netloc and full_url not in seen:
                # ã€Œæ¬¡ã¸ã€ã€Œå‰ã¸ã€ãªã©ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’é™¤å¤–
                text = link.get_text(strip=True).lower()
                if text not in ["next", "prev", "previous", "Â»", "Â«", "â€º", "â€¹", "æ¬¡ã¸", "å‰ã¸"]:
                    urls.append(full_url)
                    seen.add(full_url)

    # åŸºæœ¬URLã®ãƒ‘ã‚¹ã®é•·ã•ã‚’å–å¾—ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
    base_path = urlparse(url).path.rstrip('/')

    # URLã‚’ãƒšãƒ¼ã‚¸ç•ªå·é †ã«ã‚½ãƒ¼ãƒˆ
    def extract_page_num(u):
        path = urlparse(u).path.rstrip('/')
        # ãƒ™ãƒ¼ã‚¹URLã¨åŒã˜ãƒ‘ã‚¹ãªã‚‰1ãƒšãƒ¼ã‚¸ç›®
        if path == base_path:
            return 1
        # ãƒ™ãƒ¼ã‚¹URLã®å¾Œã«/æ•°å­—ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹: /archives/823243/2ï¼‰
        if path.startswith(base_path + '/'):
            suffix = path[len(base_path)+1:]
            if suffix.isdigit():
                return int(suffix)
        return 999  # ä¸æ˜ãªå ´åˆã¯æœ€å¾Œã«

    urls.sort(key=extract_page_num)

    if debug and len(urls) > 1:
        st.write(f"æ¤œå‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸: {len(urls)}ãƒšãƒ¼ã‚¸")
        for u in urls:
            st.write(f"  - {u}")

    return urls


def get_page_images(url: str, debug: bool = False) -> tuple[list[dict], BeautifulSoup]:
    """ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒURLã‚’æŠ½å‡º"""
    headers = get_request_headers(url)

    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
    except requests.RequestException as e:
        st.error(f"ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return [], None

    soup = BeautifulSoup(response.content, "html.parser")
    images = []

    if debug:
        st.write(f"HTMLã‚µã‚¤ã‚º: {len(response.content)} bytes")

    # è¨˜äº‹æœ¬æ–‡å†…ã®ç”»åƒã‚’å„ªå…ˆçš„ã«å–å¾—
    content_selectors = [
        "article",
        ".entry-content",
        ".post-content",
        ".article-content",
        ".content",
        ".single-content",
        ".post-body",
        ".article-body",
        "main",
        "#content",
        "#main",
        ".post",
        ".entry",
        ".ystd",
        "#ystd",
    ]

    content_area = None
    for selector in content_selectors:
        content_area = soup.select_one(selector)
        if content_area:
            if debug:
                st.write(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢æ¤œå‡º: {selector}")
            break

    if not content_area:
        content_area = soup.body if soup.body else soup
        if debug:
            st.write("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢: bodyå…¨ä½“")

    img_tags = content_area.find_all("img")

    if debug:
        st.write(f"æ¤œå‡ºã•ã‚ŒãŸimgã‚¿ã‚°æ•°: {len(img_tags)}")

    skip_patterns = [
        "icon", "logo", "avatar", "emoji", "button",
        "banner", "advertisement", "widget",
        "gravatar", "favicon", "sprite", "pixel",
        "tracking", "analytics", "1x1"
    ]

    for img in img_tags:
        src = (
            img.get("src") or
            img.get("data-src") or
            img.get("data-lazy-src") or
            img.get("data-original") or
            img.get("data-full-url") or
            img.get("srcset", "").split()[0] if img.get("srcset") else None
        )

        if not src:
            continue

        if src.startswith("data:"):
            continue

        img_url = urljoin(url, src)

        img_extensions = [".jpg", ".jpeg", ".png", ".gif", ".webp"]
        has_img_ext = any(ext in img_url.lower() for ext in img_extensions)

        if any(pattern in img_url.lower() for pattern in skip_patterns):
            continue

        alt_text = img.get("alt", "")

        if has_img_ext or "/uploads/" in img_url or "/images/" in img_url:
            images.append({
                "url": img_url,
                "alt": alt_text
            })
            if debug:
                st.write(f"ç”»åƒè¿½åŠ : {img_url[:80]}...")

    # é‡è¤‡ã‚’é™¤å»
    seen_urls = set()
    unique_images = []
    for img in images:
        if img["url"] not in seen_urls:
            seen_urls.add(img["url"])
            unique_images.append(img)

    return unique_images, soup


def get_all_pages_images(url: str, debug: bool = False) -> list[dict]:
    """ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒã‚’å–å¾—"""
    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    first_page_images, soup = get_page_images(url, debug)

    if not soup:
        return []

    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º
    page_urls = get_pagination_urls(url, soup, debug)

    all_images = []
    seen_urls = set()

    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’è¿½åŠ 
    for img in first_page_images:
        if img["url"] not in seen_urls:
            img["page"] = 1
            all_images.append(img)
            seen_urls.add(img["url"])

    # è¿½åŠ ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆ
    if len(page_urls) > 1:
        for i, page_url in enumerate(page_urls[1:], start=2):
            if debug:
                st.write(f"ãƒšãƒ¼ã‚¸ {i} ã‚’å–å¾—ä¸­: {page_url}")

            page_images, _ = get_page_images(page_url, debug)

            for img in page_images:
                if img["url"] not in seen_urls:
                    img["page"] = i
                    all_images.append(img)
                    seen_urls.add(img["url"])

    return all_images


def get_next_episode_url(soup: BeautifulSoup, base_url: str, debug: bool = False) -> str | None:
    """ã€Œæ¬¡ã®è©±>>ã€ã®URLã‚’å–å¾—"""
    # <div class="page-text-body">æ¬¡ã®è©±ï¼ï¼</div> ã‚’æ¤œå‡º
    next_episode_div = soup.find("div", class_="page-text-body", string=lambda t: t and "æ¬¡ã®è©±" in t)

    if next_episode_div:
        # è¦ªè¦ç´ ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        parent = next_episode_div.find_parent("a")
        if parent and parent.get("href"):
            next_url = urljoin(base_url, parent["href"])
            if debug:
                st.write(f"ğŸ”— æ¬¡ã®è©±ã‚’æ¤œå‡º: {next_url}")
            return next_url

        # å…„å¼Ÿè¦ç´ ã‚„è¿‘ãã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        next_link = next_episode_div.find_next("a")
        if next_link and next_link.get("href"):
            next_url = urljoin(base_url, next_link["href"])
            if debug:
                st.write(f"ğŸ”— æ¬¡ã®è©±ã‚’æ¤œå‡º: {next_url}")
            return next_url

    if debug:
        st.write("â„¹ï¸ ã€Œæ¬¡ã®è©±ã€ãƒªãƒ³ã‚¯ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    return None


def get_episode_images(url: str, episode_num: int = 1, debug: bool = False) -> tuple[list[dict], str | None]:
    """1è©±åˆ†ã®ç”»åƒã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å«ã‚€ï¼‰

    Returns:
        tuple: (ç”»åƒãƒªã‚¹ãƒˆ, æ¬¡ã®è©±ã®URL or None)
    """
    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    first_page_images, soup = get_page_images(url, debug)

    if not soup:
        return [], None

    # ã€Œæ¬¡ã®è©±ã€ã®URLã‚’å–å¾—
    next_episode_url = get_next_episode_url(soup, url, debug)

    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º
    page_urls = get_pagination_urls(url, soup, debug)

    all_images = []
    seen_urls = set()

    if debug:
        st.write(f"ğŸ“– ç¬¬{episode_num}è©±ã®å–å¾—é–‹å§‹")

    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’è¿½åŠ 
    for img in first_page_images:
        if img["url"] not in seen_urls:
            img["page"] = 1
            img["episode"] = episode_num
            all_images.append(img)
            seen_urls.add(img["url"])

    # è¿½åŠ ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆ
    if len(page_urls) > 1:
        for i, page_url in enumerate(page_urls[1:], start=2):
            if debug:
                st.write(f"  ãƒšãƒ¼ã‚¸ {i} ã‚’å–å¾—ä¸­: {page_url}")

            page_images, page_soup = get_page_images(page_url, debug)

            for img in page_images:
                if img["url"] not in seen_urls:
                    img["page"] = i
                    img["episode"] = episode_num
                    all_images.append(img)
                    seen_urls.add(img["url"])

            # å„ãƒšãƒ¼ã‚¸ã§ã‚‚ã€Œæ¬¡ã®è©±ã€ãƒªãƒ³ã‚¯ã‚’ç¢ºèªï¼ˆæœ€å¾Œã®ãƒšãƒ¼ã‚¸ã§è¦‹ã¤ã‹ã‚‹ã“ã¨ãŒã‚ã‚‹ï¼‰
            if page_soup and not next_episode_url:
                next_episode_url = get_next_episode_url(page_soup, page_url, debug)

    if debug:
        st.write(f"ğŸ“– ç¬¬{episode_num}è©±: {len(all_images)}æšã®ç”»åƒã‚’å–å¾—")

    return all_images, next_episode_url


def get_multiple_episodes_images(url: str, num_episodes: int, debug: bool = False) -> list[dict]:
    """è¤‡æ•°è©±ã®ç”»åƒã‚’å–å¾—

    Args:
        url: é–‹å§‹è©±ã®URL
        num_episodes: å–å¾—ã™ã‚‹è©±æ•°
        debug: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰

    Returns:
        list: å…¨è©±ã®ç”»åƒãƒªã‚¹ãƒˆ
    """
    all_images = []
    current_url = url

    for episode in range(1, num_episodes + 1):
        if not current_url:
            if debug:
                st.write(f"âš ï¸ ç¬¬{episode}è©±ã®URLãŒã‚ã‚Šã¾ã›ã‚“ã€‚å–å¾—ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break

        if debug:
            st.write(f"ğŸ“š ç¬¬{episode}è©±ã‚’å–å¾—ä¸­: {current_url}")

        episode_images, next_url = get_episode_images(current_url, episode_num=episode, debug=debug)
        all_images.extend(episode_images)

        # æ¬¡ã®è©±ã¸
        current_url = next_url

        if not next_url and episode < num_episodes:
            if debug:
                st.write(f"â„¹ï¸ ç¬¬{episode}è©±ãŒæœ€çµ‚è©±ã§ã™ã€‚{episode}è©±åˆ†ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
            break

    if debug:
        st.write(f"âœ… åˆè¨ˆ {len(all_images)}æšã®ç”»åƒã‚’å–å¾—ï¼ˆ{min(episode, num_episodes)}è©±åˆ†ï¼‰")

    return all_images


def download_image(url: str, referer: str = "") -> bytes | None:
    """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
        "Referer": referer,
    }

    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.content
    except requests.RequestException:
        return None


def filter_manga_images(images: list[dict], min_size: int = 50000, referer: str = "", debug: bool = False) -> list[dict]:
    """æ¼«ç”»ç”»åƒã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    manga_images = []

    for img_info in images:
        img_data = download_image(img_info["url"], referer)
        if not img_data:
            if debug:
                st.write(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {img_info['url'][:60]}...")
            continue

        if len(img_data) < min_size:
            if debug:
                st.write(f"ã‚µã‚¤ã‚ºä¸è¶³ ({len(img_data)} bytes): {img_info['url'][:60]}...")
            continue

        try:
            img = Image.open(BytesIO(img_data))
            width, height = img.size

            aspect_ratio = width / height if height > 0 else 0

            if aspect_ratio > 3:
                if debug:
                    st.write(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”é™¤å¤– ({aspect_ratio:.2f}): {img_info['url'][:60]}...")
                continue

            if width < 200 or height < 200:
                if debug:
                    st.write(f"ã‚µã‚¤ã‚ºé™¤å¤– ({width}x{height}): {img_info['url'][:60]}...")
                continue

            manga_images.append({
                **img_info,
                "data": img_data,
                "width": width,
                "height": height,
                "size": len(img_data)
            })

            if debug:
                st.write(f"âœ… æ¼«ç”»ç”»åƒã¨ã—ã¦è¿½åŠ : {width}x{height}, {len(img_data)} bytes")

        except Exception as e:
            if debug:
                st.write(f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return manga_images


def encode_image_to_base64(img_info: dict) -> tuple[str, str]:
    """ç”»åƒã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
    img = Image.open(BytesIO(img_info["data"]))
    format_map = {
        "JPEG": "image/jpeg",
        "PNG": "image/png",
        "GIF": "image/gif",
        "WEBP": "image/webp"
    }
    media_type = format_map.get(img.format, "image/jpeg")
    base64_image = base64.standard_b64encode(img_info["data"]).decode("utf-8")
    return base64_image, media_type


def extract_panel_details(images: list[dict], api_key: str, title: str = "") -> str:
    """Step1: å„ç”»åƒã®ã‚»ãƒªãƒ•ãƒ»çŠ¶æ³ã‚’è©³ç´°ã«æŠ½å‡º"""
    client = anthropic.Anthropic(api_key=api_key)

    content = []

    # ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹å ´åˆã¯å‚è€ƒæƒ…å ±ã¨ã—ã¦å…ˆã«è¿½åŠ 
    if title:
        content.append({
            "type": "text",
            "text": f"ã€å‚è€ƒï¼šè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€‘\n{title}\n\n"
        })

    for i, img_info in enumerate(images):
        base64_image, media_type = encode_image_to_base64(img_info)

        content.append({
            "type": "text",
            "text": f"ã€ç”»åƒ {i+1}ã€‘"
        })
        content.append({
            "type": "image",
            "source": {
                "type": "base64",
                "media_type": media_type,
                "data": base64_image,
            },
        })

    content.append({
        "type": "text",
        "text": """å„ç”»åƒã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ã€‘
- å¹ãå‡ºã—å†…ã®ã‚»ãƒªãƒ•ã¯ä¸€å­—ä¸€å¥æ­£ç¢ºã«æ›¸ãèµ·ã“ã—ã¦ãã ã•ã„
- èª°ãŒèª°ã«è©±ã—ã¦ã„ã‚‹ã‹æ˜ç¢ºã«ã—ã¦ãã ã•ã„
- ç™»å ´äººç‰©ã®å‘¼ã³æ–¹ï¼ˆã€ŒãŠç¾©æ¯ã•ã‚“ã€ã€Œãƒãƒã€ã€ŒãŠæ¯ã•ã‚“ã€ç­‰ï¼‰ã«æ³¨ç›®ã—ã€é–¢ä¿‚æ€§ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„
- ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®å¤–è¦‹ã®ç‰¹å¾´ï¼ˆå¹´é½¢å±¤ã€é«ªå‹ã€æœè£…ï¼‰ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„

å„ç”»åƒã«ã¤ã„ã¦ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ï¼š

### ç”»åƒX
- **ã‚»ãƒªãƒ•**: ï¼ˆå¹ãå‡ºã—å†…ã®ã‚»ãƒªãƒ•ã‚’å…¨ã¦æ›¸ãèµ·ã“ã—ã€‚èª°ã®ç™ºè¨€ã‹æ˜è¨˜ï¼‰
- **çŠ¶æ³**: ï¼ˆä½•ãŒèµ·ãã¦ã„ã‚‹ã‹ï¼‰
- **ç™»å ´äººç‰©**: ï¼ˆã“ã®ç”»åƒã«ç™»å ´ã™ã‚‹äººç‰©ã¨ç‰¹å¾´ï¼‰
- **æ„Ÿæƒ…/è¡¨æƒ…**: ï¼ˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®æ„Ÿæƒ…çŠ¶æ…‹ï¼‰"""
    })

    try:
        message = client.messages.create(
            model="claude-opus-4-5-20251101",
            max_tokens=4096,
            messages=[{"role": "user", "content": content}],
        )
        return message.content[0].text
    except Exception as e:
        return f"æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}"


def summarize_story(panel_details: str, api_key: str, title: str = "") -> str:
    """Step2: æŠ½å‡ºã—ãŸæƒ…å ±ã‹ã‚‰ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ã¾ã¨ã‚ã‚‹"""
    client = anthropic.Anthropic(api_key=api_key)

    prompt = f"""ä»¥ä¸‹ã¯æ¼«ç”»ã®å„ã‚³ãƒã‹ã‚‰æŠ½å‡ºã—ãŸã‚»ãƒªãƒ•ã¨çŠ¶æ³ã®è©³ç´°ã§ã™ã€‚
ã“ã‚Œã‚’å…ƒã«ã€ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æ­£ç¢ºã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

"""
    if title:
        prompt += f"ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€‘\n{title}\n\n"

    prompt += f"""ã€æŠ½å‡ºã•ã‚ŒãŸè©³ç´°æƒ…å ±ã€‘
{panel_details}

---

ä¸Šè¨˜ã®æƒ…å ±ã‚’å…ƒã«ã€ä»¥ä¸‹ã®å½¢å¼ã§ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š

## ã‚ã‚‰ã™ã˜
ï¼ˆã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®æµã‚Œã‚’3ã€œ5æ–‡ã§èª¬æ˜ã€‚ã‚»ãƒªãƒ•ã®å†…å®¹ã‚’åæ˜ ã—ã€ç™»å ´äººç‰©ã®é–¢ä¿‚æ€§ã‚’æ˜ç¢ºã«è¨˜è¼‰ï¼‰

## ç™»å ´äººç‰©
ï¼ˆä¸»è¦ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’ç®‡æ¡æ›¸ãã§ã€‚é–¢ä¿‚æ€§ã‚‚æ˜è¨˜ã€‚ä¾‹ï¼šã€Œç¾©æ¯ï¼ˆä¸»äººå…¬ã®å¤«ã®æ¯ï¼‰ã€ï¼‰

## ãƒã‚¤ãƒ³ãƒˆ
ï¼ˆã“ã®æ¼«ç”»ã®è¦‹ã©ã“ã‚ã‚„æ•™è¨“ã‚’1ã€œ2æ–‡ã§ï¼‰

## ä¸»è¦ãªã‚»ãƒªãƒ•
ï¼ˆã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ç†è§£ã™ã‚‹ä¸Šã§é‡è¦ãªã‚»ãƒªãƒ•ã‚’2ã€œ3å€‹å¼•ç”¨ï¼‰"""

    try:
        message = client.messages.create(
            model="claude-opus-4-5-20251101",
            max_tokens=2048,
            messages=[{"role": "user", "content": prompt}],
        )
        return message.content[0].text
    except Exception as e:
        return f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {str(e)}"


def analyze_images_batch(images: list[dict], api_key: str, title: str = "") -> str:
    """2æ®µéšè§£æ: ã‚»ãƒªãƒ•æŠ½å‡ºâ†’ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã¾ã¨ã‚"""

    # Step 1: å„ç”»åƒã®ã‚»ãƒªãƒ•ãƒ»çŠ¶æ³ã‚’è©³ç´°ã«æŠ½å‡º
    panel_details = extract_panel_details(images, api_key, title)

    if panel_details.startswith("æŠ½å‡ºã‚¨ãƒ©ãƒ¼"):
        return panel_details

    # Step 2: æŠ½å‡ºã—ãŸæƒ…å ±ã‹ã‚‰ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ã¾ã¨ã‚ã‚‹
    summary = summarize_story(panel_details, api_key, title)

    return summary


def check_title_consistency(title: str, summary: str, api_key: str) -> str:
    """ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚ã‚‰ã™ã˜ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    client = anthropic.Anthropic(api_key=api_key)

    prompt = f"""ä»¥ä¸‹ã®æ¼«ç”»è¨˜äº‹ã®ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ã¨ã€Œã‚ã‚‰ã™ã˜ã€ã‚’æ¯”è¼ƒã—ã¦ã€æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘
{title}

ã€ã‚ã‚‰ã™ã˜ã€‘
{summary}

---

ä»¥ä¸‹ã®è¦³ç‚¹ã§ãƒã‚§ãƒƒã‚¯ã—ã€çµæœã‚’å ±å‘Šã—ã¦ãã ã•ã„ï¼š

## æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯çµæœ

### åˆ¤å®š: [â—¯ æ•´åˆ / â–³ è»½å¾®ãªé•å’Œæ„Ÿ / âœ• ä¸æ•´åˆ]

### ãƒã‚§ãƒƒã‚¯é …ç›®

1. **ãƒ†ãƒ¼ãƒã®ä¸€è‡´**: ã‚¿ã‚¤ãƒˆãƒ«ãŒç¤ºã™ãƒ†ãƒ¼ãƒã¨ã‚ã‚‰ã™ã˜ã®å†…å®¹ã¯ä¸€è‡´ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ
2. **ç™»å ´äººç‰©**: ã‚¿ã‚¤ãƒˆãƒ«ã«äººç‰©ã‚„é–¢ä¿‚æ€§ãŒå«ã¾ã‚Œã‚‹å ´åˆã€ã‚ã‚‰ã™ã˜ã¨ä¸€è‡´ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ
3. **çµæœ«ãƒ»æ•™è¨“**: ã‚¿ã‚¤ãƒˆãƒ«ãŒç¤ºå”†ã™ã‚‹çµæœ«ã‚„æ•™è¨“ã¯ã€ã‚ã‚‰ã™ã˜ã«åæ˜ ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ
4. **èª‡å¤§è¡¨ç¾**: ã‚¿ã‚¤ãƒˆãƒ«ãŒå†…å®¹ã‚’èª‡å¼µã—ã™ãã¦ã„ã¾ã›ã‚“ã‹ï¼Ÿ

### è©³ç´°ã‚³ãƒ¡ãƒ³ãƒˆ
ï¼ˆé•å’Œæ„ŸãŒã‚ã‚‹å ´åˆã¯å…·ä½“çš„ã«æŒ‡æ‘˜ã—ã¦ãã ã•ã„ï¼‰

### æ”¹å–„ææ¡ˆ
ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®æ”¹å–„æ¡ˆãŒã‚ã‚Œã°ææ¡ˆã—ã¦ãã ã•ã„ï¼‰"""

    try:
        message = client.messages.create(
            model="claude-opus-4-5-20251101",
            max_tokens=1024,
            messages=[
                {"role": "user", "content": prompt}
            ],
        )
        return message.content[0].text
    except Exception as e:
        return f"ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}"


# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")

    # APIã‚­ãƒ¼ã®å–å¾—å…ƒã‚’ç¢ºèª
    env_key = get_api_key_from_env()
    secrets_key = get_api_key_from_secrets()

    if env_key:
        # .envã«è¨­å®šæ¸ˆã¿ã®å ´åˆ
        st.success("APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿ï¼ˆ.envï¼‰")
        api_key = env_key
    elif secrets_key:
        # Secretsã«è¨­å®šæ¸ˆã¿ã®å ´åˆ
        st.success("APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿ï¼ˆSecretsï¼‰")
        api_key = secrets_key
    else:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰
        api_key_input = st.text_input(
            "Anthropic API Key",
            type="password",
            value=st.session_state.get("user_api_key", ""),
            help="Claude APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            key="api_key_input"
        )

        if st.button("ğŸ” APIã‚­ãƒ¼ã‚’è¨­å®š", use_container_width=True):
            if api_key_input:
                st.session_state["user_api_key"] = api_key_input
                st.success("ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ã—ã¾ã—ãŸ")
                st.rerun()
            else:
                st.warning("APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

        if st.session_state.get("user_api_key"):
            st.info("APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰")
            if st.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢", use_container_width=True):
                del st.session_state["user_api_key"]
                st.rerun()

        api_key = st.session_state.get("user_api_key", "")

    st.divider()

    st.subheader("ç”»åƒãƒ•ã‚£ãƒ«ã‚¿è¨­å®š")
    min_image_size = st.slider(
        "æœ€å°ç”»åƒã‚µã‚¤ã‚º (KB)",
        min_value=1,
        max_value=500,
        value=30,
        help="ã“ã®å€¤ã‚ˆã‚Šå°ã•ã„ç”»åƒã¯é™¤å¤–ã•ã‚Œã¾ã™"
    )

    debug_mode = st.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰", value=True, help="ç”»åƒæ¤œå‡ºã®è©³ç´°ã‚’è¡¨ç¤º")

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
st.subheader("ğŸ“– æ¼«ç”»ã‚¿ã‚¤ãƒ—ã‚’é¸æŠã—ã¦URLã‚’å…¥åŠ›")

manga_type_col1, manga_type_col2 = st.columns(2)

with manga_type_col1:
    st.markdown("**ğŸ“š é€£è¼‰æ¼«ç”»**ï¼ˆ3è©±åˆ†èª­ã¿è¾¼ã¿ï¼‰")
    serial_url = st.text_input(
        "é€£è¼‰æ¼«ç”»URL",
        placeholder="https://example.com/serial-manga",
        help="é€£è¼‰æ¼«ç”»ã®URLã‚’å…¥åŠ›ï¼ˆ3è©±åˆ†ã®ç”»åƒã‚’èª­ã¿è¾¼ã¿ã¾ã™ï¼‰",
        label_visibility="collapsed"
    )

with manga_type_col2:
    st.markdown("**ğŸ“„ ã‚¨ãƒ”æ¼«ç”»**ï¼ˆ1è©±åˆ†èª­ã¿è¾¼ã¿ï¼‰")
    episode_url = st.text_input(
        "ã‚¨ãƒ”æ¼«ç”»URL",
        placeholder="https://example.com/episode-manga",
        help="ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¼«ç”»ã®URLã‚’å…¥åŠ›ï¼ˆ1è©±åˆ†ã®ç”»åƒã‚’èª­ã¿è¾¼ã¿ã¾ã™ï¼‰",
        label_visibility="collapsed"
    )

# ã©ã¡ã‚‰ã®URLãŒå…¥åŠ›ã•ã‚ŒãŸã‹åˆ¤å®š
url = ""
num_episodes = 1
manga_type_label = ""

if serial_url and episode_url:
    st.warning("âš ï¸ ã©ã¡ã‚‰ã‹ä¸€æ–¹ã®URLã®ã¿å…¥åŠ›ã—ã¦ãã ã•ã„")
elif serial_url:
    url = serial_url
    num_episodes = 3
    manga_type_label = "é€£è¼‰æ¼«ç”»"
elif episode_url:
    url = episode_url
    num_episodes = 1
    manga_type_label = "ã‚¨ãƒ”æ¼«ç”»"

article_title = st.text_input(
    "ğŸ“° è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä»»æ„ï¼‰",
    placeholder="æ¼«ç”»è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›",
    help="ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã‚ã‚‰ã™ã˜ã¨ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™"
)

col1, col2 = st.columns([1, 4])
with col1:
    analyze_button = st.button("ğŸ” è§£æé–‹å§‹", type="primary", use_container_width=True)

if analyze_button:
    if not url:
        st.error("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆé€£è¼‰æ¼«ç”»ã¾ãŸã¯ã‚¨ãƒ”æ¼«ç”»ã®ã©ã¡ã‚‰ã‹ï¼‰")
    elif serial_url and episode_url:
        st.error("ã©ã¡ã‚‰ã‹ä¸€æ–¹ã®URLã®ã¿å…¥åŠ›ã—ã¦ãã ã•ã„")
    elif not api_key:
        st.error("APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    else:
        st.info(f"ğŸ“– **{manga_type_label}**ã¨ã—ã¦è§£æã—ã¾ã™ï¼ˆ{num_episodes}è©±åˆ†ï¼‰")

        with st.spinner("ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒã‚’å–å¾—ä¸­..."):
            # æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯: è©±æ•°å˜ä½ã§å–å¾—
            images = get_multiple_episodes_images(url, num_episodes=num_episodes, debug=debug_mode)

        if not images:
            st.warning("ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’ONã«ã—ã¦è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            st.info(f"ğŸ“· {len(images)}ä»¶ã®ç”»åƒã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚æ¼«ç”»ç”»åƒã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...")

            with st.spinner("æ¼«ç”»ç”»åƒã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­..."):
                manga_images = filter_manga_images(
                    images,
                    min_size=min_image_size * 1000,
                    referer=url,
                    debug=debug_mode
                )

            if not manga_images:
                st.warning("æ¼«ç”»ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šã‚’èª¿æ•´ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")

                if debug_mode and images:
                    st.subheader("æ¤œå‡ºã•ã‚ŒãŸç”»åƒURLä¸€è¦§")
                    for img in images:
                        st.text(img["url"])
            else:
                # è©±æ•°ã”ã¨ã®ç”»åƒæ•°ã‚’é›†è¨ˆ
                episode_counts = {}
                for img in manga_images:
                    ep = img.get("episode", 1)
                    episode_counts[ep] = episode_counts.get(ep, 0) + 1

                episode_summary = "ã€".join([f"ç¬¬{ep}è©±: {count}æš" for ep, count in sorted(episode_counts.items())])
                st.success(f"ğŸ“š {len(manga_images)}ä»¶ã®æ¼«ç”»ç”»åƒã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼ˆ{episode_summary}ï¼‰")

                # ç”»åƒã‚’è¡¨ç¤º
                st.header("ğŸ–¼ï¸ æ¤œå‡ºã•ã‚ŒãŸæ¼«ç”»ç”»åƒ")

                # ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºï¼ˆè©±æ•°ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰
                cols_per_row = 3
                for i in range(0, len(manga_images), cols_per_row):
                    cols = st.columns(cols_per_row)
                    for j, col in enumerate(cols):
                        idx = i + j
                        if idx < len(manga_images):
                            img_info = manga_images[idx]
                            with col:
                                page_num = img_info.get("page", 1)
                                episode_num = img_info.get("episode", 1)
                                st.image(
                                    img_info["data"],
                                    caption=f"ç¬¬{episode_num}è©± P{page_num}",
                                    use_container_width=True
                                )

                # ã‚ã‚‰ã™ã˜è§£æ
                st.divider()
                st.header("ğŸ“ ã‚ã‚‰ã™ã˜è§£æ")

                with st.spinner("AIãŒã‚ã‚‰ã™ã˜ã‚’è§£æä¸­..."):
                    summary = analyze_images_batch(manga_images, api_key, title=article_title)

                st.markdown(summary)

                # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                if article_title:
                    st.divider()
                    st.header("ğŸ” ã‚¿ã‚¤ãƒˆãƒ«æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯")

                    with st.spinner("ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚ã‚‰ã™ã˜ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ä¸­..."):
                        consistency_result = check_title_consistency(article_title, summary, api_key)

                    st.markdown(consistency_result)

# ãƒ•ãƒƒã‚¿ãƒ¼
st.divider()
st.caption("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã‚ã‚‰ã™ã˜ã¨ã®æ•´åˆæ€§ã‚’è‡ªå‹•ãƒã‚§ãƒƒã‚¯ã—ã¾ã™")

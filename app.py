import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import anthropic
import base64
from io import BytesIO
from PIL import Image
import re


def get_api_key_from_secrets() -> str:
    """Streamlit Secretsã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—"""
    try:
        return st.secrets.get("ANTHROPIC_API_KEY", "")
    except Exception:
        return ""


def get_stored_api_key() -> str:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¾ãŸã¯Secretsã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—"""
    # ã¾ãšSecretsã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç®¡ç†è€…è¨­å®šï¼‰
    secrets_key = get_api_key_from_secrets()
    if secrets_key:
        return secrets_key
    # æ¬¡ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼‰
    return st.session_state.get("user_api_key", "")


st.set_page_config(
    page_title="æ¼«ç”»ç”»åƒè§£æãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ“š",
    layout="wide"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .stCheckbox p {
        font-size: 14px;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ“š æ¼«ç”»ç”»åƒè§£æãƒ„ãƒ¼ãƒ«")
st.markdown("URLã‹ã‚‰æ¼«ç”»ç”»åƒã‚’æŠ½å‡ºã—ã€AIã§ã‚ã‚‰ã™ã˜ã‚’è§£æã—ã¾ã™")


def get_request_headers(url: str) -> dict:
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
    parsed_url = urlparse(url)
    base_domain = f"{parsed_url.scheme}://{parsed_url.netloc}"
    return {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Referer": base_domain,
    }


def get_pagination_urls(url: str, soup: BeautifulSoup, debug: bool = False) -> list[str]:
    """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®URLã‚’å–å¾—"""
    urls = [url]  # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã‚’å«ã‚€

    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³
    pagination_selectors = [
        ".pagination a",
        ".page-numbers a",
        ".pager a",
        ".wp-pagenavi a",
        "nav.navigation a",
        ".post-page-numbers",
        # æ•°å­—ãƒªãƒ³ã‚¯ï¼ˆ1, 2, 3...ï¼‰
        "a.page-link",
        ".pages a",
    ]

    pagination_links = []

    for selector in pagination_selectors:
        links = soup.select(selector)
        if links:
            pagination_links.extend(links)
            if debug:
                st.write(f"ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º: {selector} ({len(links)}ä»¶)")
            break

    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€æ•°å­—ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
    if not pagination_links:
        # ãƒ†ã‚­ã‚¹ãƒˆãŒæ•°å­—ã®ã¿ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        all_links = soup.find_all("a")
        base_path = urlparse(url).path.rstrip('/')
        for link in all_links:
            text = link.get_text(strip=True)
            href = link.get("href", "")
            if not href:
                continue
            # æ•°å­—ã®ã¿ã®ãƒ†ã‚­ã‚¹ãƒˆ
            if text.isdigit():
                # çµ¶å¯¾URLã¾ãŸã¯ç›¸å¯¾URLã§åŒã˜è¨˜äº‹ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
                full_href = urljoin(url, href)
                href_path = urlparse(full_href).path.rstrip('/')
                # åŒã˜è¨˜äº‹ã¸ã®ãƒªãƒ³ã‚¯ï¼ˆ/archives/823243/2 å½¢å¼ï¼‰
                if href_path.startswith(base_path):
                    pagination_links.append(link)
                    if debug:
                        st.write(f"æ•°å­—ãƒªãƒ³ã‚¯æ¤œå‡º: {text} -> {full_href}")

    # URLã‚’æŠ½å‡º
    seen = {url}
    for link in pagination_links:
        href = link.get("href")
        if href:
            full_url = urljoin(url, href)
            # åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã€ã¾ã è¿½åŠ ã•ã‚Œã¦ã„ãªã„URL
            if urlparse(full_url).netloc == urlparse(url).netloc and full_url not in seen:
                # ã€Œæ¬¡ã¸ã€ã€Œå‰ã¸ã€ãªã©ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’é™¤å¤–
                text = link.get_text(strip=True).lower()
                if text not in ["next", "prev", "previous", "Â»", "Â«", "â€º", "â€¹", "æ¬¡ã¸", "å‰ã¸"]:
                    urls.append(full_url)
                    seen.add(full_url)

    # åŸºæœ¬URLã®ãƒ‘ã‚¹ã®é•·ã•ã‚’å–å¾—ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
    base_path = urlparse(url).path.rstrip('/')

    # URLã‚’ãƒšãƒ¼ã‚¸ç•ªå·é †ã«ã‚½ãƒ¼ãƒˆ
    def extract_page_num(u):
        path = urlparse(u).path.rstrip('/')
        # ãƒ™ãƒ¼ã‚¹URLã¨åŒã˜ãƒ‘ã‚¹ãªã‚‰1ãƒšãƒ¼ã‚¸ç›®
        if path == base_path:
            return 1
        # ãƒ™ãƒ¼ã‚¹URLã®å¾Œã«/æ•°å­—ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹: /archives/823243/2ï¼‰
        if path.startswith(base_path + '/'):
            suffix = path[len(base_path)+1:]
            if suffix.isdigit():
                return int(suffix)
        return 999  # ä¸æ˜ãªå ´åˆã¯æœ€å¾Œã«

    urls.sort(key=extract_page_num)

    if debug and len(urls) > 1:
        st.write(f"æ¤œå‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸: {len(urls)}ãƒšãƒ¼ã‚¸")
        for u in urls:
            st.write(f"  - {u}")

    return urls


def get_page_images(url: str, debug: bool = False) -> tuple[list[dict], BeautifulSoup]:
    """ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒURLã‚’æŠ½å‡º"""
    headers = get_request_headers(url)

    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
    except requests.RequestException as e:
        st.error(f"ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return [], None

    soup = BeautifulSoup(response.content, "html.parser")
    images = []

    if debug:
        st.write(f"HTMLã‚µã‚¤ã‚º: {len(response.content)} bytes")

    # è¨˜äº‹æœ¬æ–‡å†…ã®ç”»åƒã‚’å„ªå…ˆçš„ã«å–å¾—
    content_selectors = [
        "article",
        ".entry-content",
        ".post-content",
        ".article-content",
        ".content",
        ".single-content",
        ".post-body",
        ".article-body",
        "main",
        "#content",
        "#main",
        ".post",
        ".entry",
        ".ystd",
        "#ystd",
    ]

    content_area = None
    for selector in content_selectors:
        content_area = soup.select_one(selector)
        if content_area:
            if debug:
                st.write(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢æ¤œå‡º: {selector}")
            break

    if not content_area:
        content_area = soup.body if soup.body else soup
        if debug:
            st.write("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢: bodyå…¨ä½“")

    img_tags = content_area.find_all("img")

    if debug:
        st.write(f"æ¤œå‡ºã•ã‚ŒãŸimgã‚¿ã‚°æ•°: {len(img_tags)}")

    skip_patterns = [
        "icon", "logo", "avatar", "emoji", "button",
        "banner", "advertisement", "widget",
        "gravatar", "favicon", "sprite", "pixel",
        "tracking", "analytics", "1x1"
    ]

    for img in img_tags:
        src = (
            img.get("src") or
            img.get("data-src") or
            img.get("data-lazy-src") or
            img.get("data-original") or
            img.get("data-full-url") or
            img.get("srcset", "").split()[0] if img.get("srcset") else None
        )

        if not src:
            continue

        if src.startswith("data:"):
            continue

        img_url = urljoin(url, src)

        img_extensions = [".jpg", ".jpeg", ".png", ".gif", ".webp"]
        has_img_ext = any(ext in img_url.lower() for ext in img_extensions)

        if any(pattern in img_url.lower() for pattern in skip_patterns):
            continue

        alt_text = img.get("alt", "")

        if has_img_ext or "/uploads/" in img_url or "/images/" in img_url:
            images.append({
                "url": img_url,
                "alt": alt_text
            })
            if debug:
                st.write(f"ç”»åƒè¿½åŠ : {img_url[:80]}...")

    # é‡è¤‡ã‚’é™¤å»
    seen_urls = set()
    unique_images = []
    for img in images:
        if img["url"] not in seen_urls:
            seen_urls.add(img["url"])
            unique_images.append(img)

    return unique_images, soup


def get_all_pages_images(url: str, debug: bool = False) -> list[dict]:
    """ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒã‚’å–å¾—"""
    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    first_page_images, soup = get_page_images(url, debug)

    if not soup:
        return []

    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º
    page_urls = get_pagination_urls(url, soup, debug)

    all_images = []
    seen_urls = set()

    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’è¿½åŠ 
    for img in first_page_images:
        if img["url"] not in seen_urls:
            img["page"] = 1
            all_images.append(img)
            seen_urls.add(img["url"])

    # è¿½åŠ ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆ
    if len(page_urls) > 1:
        for i, page_url in enumerate(page_urls[1:], start=2):
            if debug:
                st.write(f"ãƒšãƒ¼ã‚¸ {i} ã‚’å–å¾—ä¸­: {page_url}")

            page_images, _ = get_page_images(page_url, debug)

            for img in page_images:
                if img["url"] not in seen_urls:
                    img["page"] = i
                    all_images.append(img)
                    seen_urls.add(img["url"])

    return all_images


def download_image(url: str, referer: str = "") -> bytes | None:
    """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
        "Referer": referer,
    }

    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.content
    except requests.RequestException:
        return None


def filter_manga_images(images: list[dict], min_size: int = 50000, referer: str = "", debug: bool = False) -> list[dict]:
    """æ¼«ç”»ç”»åƒã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    manga_images = []

    for img_info in images:
        img_data = download_image(img_info["url"], referer)
        if not img_data:
            if debug:
                st.write(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {img_info['url'][:60]}...")
            continue

        if len(img_data) < min_size:
            if debug:
                st.write(f"ã‚µã‚¤ã‚ºä¸è¶³ ({len(img_data)} bytes): {img_info['url'][:60]}...")
            continue

        try:
            img = Image.open(BytesIO(img_data))
            width, height = img.size

            aspect_ratio = width / height if height > 0 else 0

            if aspect_ratio > 3:
                if debug:
                    st.write(f"ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”é™¤å¤– ({aspect_ratio:.2f}): {img_info['url'][:60]}...")
                continue

            if width < 200 or height < 200:
                if debug:
                    st.write(f"ã‚µã‚¤ã‚ºé™¤å¤– ({width}x{height}): {img_info['url'][:60]}...")
                continue

            manga_images.append({
                **img_info,
                "data": img_data,
                "width": width,
                "height": height,
                "size": len(img_data)
            })

            if debug:
                st.write(f"âœ… æ¼«ç”»ç”»åƒã¨ã—ã¦è¿½åŠ : {width}x{height}, {len(img_data)} bytes")

        except Exception as e:
            if debug:
                st.write(f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return manga_images


def analyze_images_batch(images: list[dict], api_key: str) -> str:
    """è¤‡æ•°ã®ç”»åƒã‚’ã¾ã¨ã‚ã¦è§£æã—ã¦ã‚ã‚‰ã™ã˜ã‚’æŠ½å‡º"""
    client = anthropic.Anthropic(api_key=api_key)

    # ç”»åƒã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆ
    content = []

    for i, img_info in enumerate(images):
        img = Image.open(BytesIO(img_info["data"]))
        format_map = {
            "JPEG": "image/jpeg",
            "PNG": "image/png",
            "GIF": "image/gif",
            "WEBP": "image/webp"
        }
        media_type = format_map.get(img.format, "image/jpeg")
        base64_image = base64.standard_b64encode(img_info["data"]).decode("utf-8")

        content.append({
            "type": "text",
            "text": f"ã€ç”»åƒ {i+1}ã€‘"
        })
        content.append({
            "type": "image",
            "source": {
                "type": "base64",
                "media_type": media_type,
                "data": base64_image,
            },
        })

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
    content.append({
        "type": "text",
        "text": """ä¸Šè¨˜ã®æ¼«ç”»ç”»åƒã‚’é †ç•ªã«è¦‹ã¦ã€ã“ã®ãƒãƒ³ã‚¬ã®ã‚ã‚‰ã™ã˜ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®å½¢å¼ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š

## ã‚ã‚‰ã™ã˜
ï¼ˆã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®æµã‚Œã‚’3ã€œ5æ–‡ã§èª¬æ˜ï¼‰

## ç™»å ´äººç‰©
ï¼ˆä¸»è¦ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’ç®‡æ¡æ›¸ãã§ï¼‰

## ãƒã‚¤ãƒ³ãƒˆ
ï¼ˆã“ã®æ¼«ç”»ã®è¦‹ã©ã“ã‚ã‚„æ•™è¨“ã‚’1ã€œ2æ–‡ã§ï¼‰

æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ã‚»ãƒªãƒ•ãŒã‚ã‚‹å ´åˆã¯ã€é‡è¦ãªã‚»ãƒªãƒ•ã‚‚å«ã‚ã¦ãã ã•ã„ã€‚"""
    })

    try:
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2048,
            messages=[
                {
                    "role": "user",
                    "content": content,
                }
            ],
        )
        return message.content[0].text
    except Exception as e:
        return f"è§£æã‚¨ãƒ©ãƒ¼: {str(e)}"


def check_title_consistency(title: str, summary: str, api_key: str) -> str:
    """ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚ã‚‰ã™ã˜ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    client = anthropic.Anthropic(api_key=api_key)

    prompt = f"""ä»¥ä¸‹ã®æ¼«ç”»è¨˜äº‹ã®ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ã¨ã€Œã‚ã‚‰ã™ã˜ã€ã‚’æ¯”è¼ƒã—ã¦ã€æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘
{title}

ã€ã‚ã‚‰ã™ã˜ã€‘
{summary}

---

ä»¥ä¸‹ã®è¦³ç‚¹ã§ãƒã‚§ãƒƒã‚¯ã—ã€çµæœã‚’å ±å‘Šã—ã¦ãã ã•ã„ï¼š

## æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯çµæœ

### åˆ¤å®š: [â—¯ æ•´åˆ / â–³ è»½å¾®ãªé•å’Œæ„Ÿ / âœ• ä¸æ•´åˆ]

### ãƒã‚§ãƒƒã‚¯é …ç›®

1. **ãƒ†ãƒ¼ãƒã®ä¸€è‡´**: ã‚¿ã‚¤ãƒˆãƒ«ãŒç¤ºã™ãƒ†ãƒ¼ãƒã¨ã‚ã‚‰ã™ã˜ã®å†…å®¹ã¯ä¸€è‡´ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ
2. **ç™»å ´äººç‰©**: ã‚¿ã‚¤ãƒˆãƒ«ã«äººç‰©ã‚„é–¢ä¿‚æ€§ãŒå«ã¾ã‚Œã‚‹å ´åˆã€ã‚ã‚‰ã™ã˜ã¨ä¸€è‡´ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ
3. **çµæœ«ãƒ»æ•™è¨“**: ã‚¿ã‚¤ãƒˆãƒ«ãŒç¤ºå”†ã™ã‚‹çµæœ«ã‚„æ•™è¨“ã¯ã€ã‚ã‚‰ã™ã˜ã«åæ˜ ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ
4. **èª‡å¤§è¡¨ç¾**: ã‚¿ã‚¤ãƒˆãƒ«ãŒå†…å®¹ã‚’èª‡å¼µã—ã™ãã¦ã„ã¾ã›ã‚“ã‹ï¼Ÿ

### è©³ç´°ã‚³ãƒ¡ãƒ³ãƒˆ
ï¼ˆé•å’Œæ„ŸãŒã‚ã‚‹å ´åˆã¯å…·ä½“çš„ã«æŒ‡æ‘˜ã—ã¦ãã ã•ã„ï¼‰

### æ”¹å–„ææ¡ˆ
ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®æ”¹å–„æ¡ˆãŒã‚ã‚Œã°ææ¡ˆã—ã¦ãã ã•ã„ï¼‰"""

    try:
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1024,
            messages=[
                {"role": "user", "content": prompt}
            ],
        )
        return message.content[0].text
    except Exception as e:
        return f"ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}"


# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")

    # Secretsã«APIã‚­ãƒ¼ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    secrets_key = get_api_key_from_secrets()

    if secrets_key:
        # Secretsã«è¨­å®šæ¸ˆã¿ã®å ´åˆ
        st.success("APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿ï¼ˆç®¡ç†è€…ï¼‰")
        api_key = secrets_key
    else:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰
        api_key_input = st.text_input(
            "Anthropic API Key",
            type="password",
            value=st.session_state.get("user_api_key", ""),
            help="Claude APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            key="api_key_input"
        )

        if st.button("ğŸ” APIã‚­ãƒ¼ã‚’è¨­å®š", use_container_width=True):
            if api_key_input:
                st.session_state["user_api_key"] = api_key_input
                st.success("ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ã—ã¾ã—ãŸ")
                st.rerun()
            else:
                st.warning("APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

        if st.session_state.get("user_api_key"):
            st.info("APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰")
            if st.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢", use_container_width=True):
                del st.session_state["user_api_key"]
                st.rerun()

        api_key = st.session_state.get("user_api_key", "")

    st.divider()

    st.subheader("ç”»åƒãƒ•ã‚£ãƒ«ã‚¿è¨­å®š")
    min_image_size = st.slider(
        "æœ€å°ç”»åƒã‚µã‚¤ã‚º (KB)",
        min_value=1,
        max_value=500,
        value=30,
        help="ã“ã®å€¤ã‚ˆã‚Šå°ã•ã„ç”»åƒã¯é™¤å¤–ã•ã‚Œã¾ã™"
    )

    auto_pagination = st.checkbox(
        "ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è‡ªå‹•æ¤œå‡º",
        value=True,
        help="è¤‡æ•°ãƒšãƒ¼ã‚¸ã®è¨˜äº‹ã‚’è‡ªå‹•ã§æ¤œå‡ºã—ã¦å…¨ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’å–å¾—"
    )

    debug_mode = st.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰", value=True, help="ç”»åƒæ¤œå‡ºã®è©³ç´°ã‚’è¡¨ç¤º")

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
url = st.text_input(
    "ğŸ”— è§£æã™ã‚‹URLã‚’å…¥åŠ›",
    placeholder="https://example.com/manga-article",
    help="æ¼«ç”»ç”»åƒãŒæ²è¼‰ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
)

article_title = st.text_input(
    "ğŸ“° è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä»»æ„ï¼‰",
    placeholder="æ¼«ç”»è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›",
    help="ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã‚ã‚‰ã™ã˜ã¨ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™"
)

col1, col2 = st.columns([1, 4])
with col1:
    analyze_button = st.button("ğŸ” è§£æé–‹å§‹", type="primary", use_container_width=True)

if analyze_button:
    if not url:
        st.error("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    elif not api_key:
        st.error("APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    else:
        with st.spinner("ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒã‚’å–å¾—ä¸­..."):
            if auto_pagination:
                images = get_all_pages_images(url, debug=debug_mode)
            else:
                images, _ = get_page_images(url, debug=debug_mode)

        if not images:
            st.warning("ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’ONã«ã—ã¦è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            st.info(f"ğŸ“· {len(images)}ä»¶ã®ç”»åƒã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚æ¼«ç”»ç”»åƒã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...")

            with st.spinner("æ¼«ç”»ç”»åƒã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­..."):
                manga_images = filter_manga_images(
                    images,
                    min_size=min_image_size * 1000,
                    referer=url,
                    debug=debug_mode
                )

            if not manga_images:
                st.warning("æ¼«ç”»ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šã‚’èª¿æ•´ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")

                if debug_mode and images:
                    st.subheader("æ¤œå‡ºã•ã‚ŒãŸç”»åƒURLä¸€è¦§")
                    for img in images:
                        st.text(img["url"])
            else:
                st.success(f"ğŸ“š {len(manga_images)}ä»¶ã®æ¼«ç”»ç”»åƒã‚’æ¤œå‡ºã—ã¾ã—ãŸ")

                # ç”»åƒã‚’è¡¨ç¤º
                st.header("ğŸ–¼ï¸ æ¤œå‡ºã•ã‚ŒãŸæ¼«ç”»ç”»åƒ")

                # ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º
                cols_per_row = 3
                for i in range(0, len(manga_images), cols_per_row):
                    cols = st.columns(cols_per_row)
                    for j, col in enumerate(cols):
                        idx = i + j
                        if idx < len(manga_images):
                            img_info = manga_images[idx]
                            with col:
                                page_num = img_info.get("page", 1)
                                st.image(
                                    img_info["data"],
                                    caption=f"ç”»åƒ {idx+1} (P{page_num})",
                                    use_container_width=True
                                )

                # ã‚ã‚‰ã™ã˜è§£æ
                st.divider()
                st.header("ğŸ“ ã‚ã‚‰ã™ã˜è§£æ")

                with st.spinner("AIãŒã‚ã‚‰ã™ã˜ã‚’è§£æä¸­..."):
                    summary = analyze_images_batch(manga_images, api_key)

                st.markdown(summary)

                # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                if article_title:
                    st.divider()
                    st.header("ğŸ” ã‚¿ã‚¤ãƒˆãƒ«æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯")

                    with st.spinner("ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚ã‚‰ã™ã˜ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ä¸­..."):
                        consistency_result = check_title_consistency(article_title, summary, api_key)

                    st.markdown(consistency_result)

# ãƒ•ãƒƒã‚¿ãƒ¼
st.divider()
st.caption("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã‚ã‚‰ã™ã˜ã¨ã®æ•´åˆæ€§ã‚’è‡ªå‹•ãƒã‚§ãƒƒã‚¯ã—ã¾ã™")
